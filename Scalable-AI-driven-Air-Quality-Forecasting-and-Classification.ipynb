{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd5_Ye_8ZrLf"
   },
   "source": [
    "**Air Quality Prediction and Classification:** <br> `Brief Info of the project:` <br>\n",
    "The aim of this project is to develop a robust and intelligent system for air quality prediction and classification that integrates data from diverse sources, such as sensors and OpenWeather API. By employing advanced preprocessing techniques, feature alignment, and reinforcement learning, the system ensures compatibility and adaptability to dynamic data variations. The project further incorporates hybrid machine learning models for accurate prediction of air quality indices and categorization into health-impactful classifications. Ultimately, the solution aims to provide real-time insights to policymakers and general users, promoting informed decision-making and fostering healthier communities. <br>\n",
    "`Dataset Info:` <br>\n",
    "The data is collected realtime from OpenWhether using API. This dataset contains 13 features and most of them are numerical type. you can have more info about the resource in the link: https://openweathermap.org. <br>\n",
    "In addition, we have gathered local data from the National Environmental Protection Agency of Afghanistan in Kabul city. And total it has 385 records from 3 different years (2024,2023,2022).\n",
    "**Deep Learning Pipeline:**<br>\n",
    "* Preprocessing.\n",
    "* Data Cleaning.\n",
    "* Feature Engineering.\n",
    "* Model Selection.\n",
    "* Model Evaluation.\n",
    "* Deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l14tKZ8uy9kD"
   },
   "outputs": [],
   "source": [
    "# necessory libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kAU49-Fg0mC"
   },
   "source": [
    "### Preprocessing:\n",
    "We have collected three dataset one from local government and the rest two others are belongs to Openweather API. This section contains the following steps.\n",
    "* Data concatination\n",
    "* Moving `local_time` from the end of columns to before aqi feature\n",
    "* Null values\n",
    "* Duplicate values\n",
    "* Check Uniqueness of Countries and States\n",
    "* Checking statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "l0RtRiBqX8hC",
    "outputId": "658c245c-c8d5-4615-d452-3eaf62b7f7a2"
   },
   "outputs": [],
   "source": [
    "# Kabul local data\n",
    "kabul = pd.read_csv(\"This was Afghanistan local data\")\n",
    "kabul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UoCzByvYDXe"
   },
   "outputs": [],
   "source": [
    "# openweather dataset\n",
    "# It has two dataset and first we need to combine and then start preprocessing steps\n",
    "first_data = pd.read_csv(\"add the dataset\")\n",
    "second_data = pd.read_csv(\"add the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2VkeH8g0daO4",
    "outputId": "e91c9388-846e-4fab-fa52-7c445c87dfec"
   },
   "outputs": [],
   "source": [
    "first_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBrHy5sonxu6",
    "outputId": "56125eff-f5b6-4417-b2b9-895e49d4f62c"
   },
   "outputs": [],
   "source": [
    "# first data info\n",
    "first_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-i9fBMVCn3oh",
    "outputId": "4c5bf331-e3b0-41a7-9ec2-088b3966f55d"
   },
   "outputs": [],
   "source": [
    "# second data head\n",
    "second_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvdedhTXoFxR",
    "outputId": "34420737-a8bd-43b8-bdf1-67b5ff8f6f38"
   },
   "outputs": [],
   "source": [
    "# second data info\n",
    "second_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-JQeAaHRopl4",
    "outputId": "b805f1cd-1d3d-4e0e-9bb5-7b36d3ec75be"
   },
   "outputs": [],
   "source": [
    "# Move 'local_time' column to before 'aqi'. we need the feature to be the same position for concatination\n",
    "columns = second_data.columns.tolist()\n",
    "columns.remove('local_time')\n",
    "aqi_index = columns.index('aqi')\n",
    "columns.insert(aqi_index, 'local_time')\n",
    "df = second_data[columns]\n",
    "\n",
    "# lets Check the new column order\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "lOwvLLDCtjyw",
    "outputId": "07bc8fef-fc65-4dbf-9410-0afc7142fbe7"
   },
   "outputs": [],
   "source": [
    "# now our two dataset is ready to be combined into a single data\n",
    "data = pd.concat([df, first_data], ignore_index=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "d7ln-wVAtlw-",
    "outputId": "dbd81ba0-1040-4c29-8fe0-c48ea137022b"
   },
   "outputs": [],
   "source": [
    "# checking total Afghanistan records before concatination with local data\n",
    "data[data['country'] == \"Afghanistan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el9kVwY4vnaS",
    "outputId": "756d22d4-03a7-46ae-9716-dec1a8434b29"
   },
   "outputs": [],
   "source": [
    "# total countries records in the dataset\n",
    "np.array(data['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ZO5a1puHwjn5",
    "outputId": "d8547de4-1cf6-4c53-a482-366d5d6dd271"
   },
   "outputs": [],
   "source": [
    "# Concatinating Kabul dataset with main data\n",
    "data = pd.concat([data,kabul], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FapEy4E5m3_6",
    "outputId": "8e837d17-a306-4311-a3f0-fa5bf149ea5f"
   },
   "outputs": [],
   "source": [
    "# main data info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Zvq7kA4jgeKj",
    "outputId": "922bc21e-68f3-47b9-dd15-7b85b9c629cf"
   },
   "outputs": [],
   "source": [
    "# Afghanistan records after concatination\n",
    "data[data['country'] == 'Afghanistan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "cqxulF3lgnWl",
    "outputId": "d137101c-5eac-4fed-d6a6-e5e068d04bb6"
   },
   "outputs": [],
   "source": [
    "# ploting total countries records\n",
    "fig = px.scatter(data['country'].value_counts())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iWfwQKuj1cU",
    "outputId": "1d9da10b-4f5f-4e16-a735-d70a0f94f2cc"
   },
   "outputs": [],
   "source": [
    "# total countries records in the dataset\n",
    "np.array(data['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "rzSF3HICnXtH",
    "outputId": "8a05a374-da91-4588-e485-2461a6147a8d"
   },
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsuOkogmnd2J"
   },
   "source": [
    "**Null values Result:**\n",
    "Totally we have null values in three main features `(aqi, no, nh3)`.<br>\n",
    "Let's find out null values in visualizaiton. <br>\n",
    "We will handle this efficently in Data Cleaning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KoBXrfuQn_MF",
    "outputId": "2d7af19f-0b3f-47fb-85c4-0719cd3b4702"
   },
   "outputs": [],
   "source": [
    "# Bar plot of missing values for each column\n",
    "drop = ['country', 'state', 'local_time','longitude', 'latitude']\n",
    "missing_values = data.drop(drop, axis=1).isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values.sort_values(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "missing_values.plot(kind='barh', color='orange')\n",
    "plt.title('Missing Values Count by Column', fontsize=12)\n",
    "plt.xlabel('Number of Missing Values', fontsize=10)\n",
    "plt.ylabel('Columns', fontsize=10)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuftvVUNtuBz",
    "outputId": "7f51fe4c-a5e1-4c87-d568-6e2e20093d53"
   },
   "outputs": [],
   "source": [
    "# Checking duplicate values\n",
    "duplicates = data.duplicated()\n",
    "print(f\"Total duplicates: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "G-qQHQbnxLeG",
    "outputId": "61ae8191-89bc-449a-a1d0-02eb3154fa2d"
   },
   "outputs": [],
   "source": [
    "# Checking inside dataframe which records have the most\n",
    "data[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZ7VqqNBx2vk",
    "outputId": "afe99d7f-7152-4e32-8804-12928076eef2"
   },
   "outputs": [],
   "source": [
    "# checking duplicates without some features\n",
    "drop = ['country', 'state','aqi','longitude', 'latitude']\n",
    "duplicates = data.drop(drop, axis=1).duplicated()\n",
    "print(f\"Total duplicates: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1lftwGqzrFY",
    "outputId": "6caf6861-a6cc-419f-8fa0-a6988cc5aaa2"
   },
   "outputs": [],
   "source": [
    "# lets check duplicates for each features individualy\n",
    "for feature in data.columns:\n",
    "  dup = data[feature].duplicated().sum()\n",
    "  print(f\"Total {feature} duplicates: {dup}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxBk5G2Rvg2p",
    "outputId": "d5e61b61-c550-4382-8388-9bc63d6e54c5"
   },
   "outputs": [],
   "source": [
    "# Check Unique values of countries\n",
    "np.array(data['country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "WrpYuJQLv-Mr",
    "outputId": "8fb67202-2f22-4cbf-cc35-fbd69d68b8fe"
   },
   "outputs": [],
   "source": [
    "# ploting the countries using plotly express\n",
    "fig = px.scatter(data['country'].unique())\n",
    "fig.update_layout(\n",
    "    xaxis_title='Number of Countries avaliable',\n",
    "    yaxis_title='Some First Countries Name'\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTV5zLVhxLaJ",
    "outputId": "df7cd39a-015d-4276-da66-23de883e3f32"
   },
   "outputs": [],
   "source": [
    "# Check Unique states\n",
    "np.array(data['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "zMnL2p-Kkm6T",
    "outputId": "0001ee91-63dc-448d-cc6a-529e6a0f9cc8"
   },
   "outputs": [],
   "source": [
    "# Checking Statistical Analysis\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-UauezYflXk1",
    "outputId": "e4123900-c471-427c-e1c8-6e36cc234a04"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# Checking the statistical anlysis using Normal distribution plot for every feature\n",
    "# Visualizing Statistacal Analysis without object types and [latitude, longitude, aqi, ...] features\n",
    "drop = ['country', 'state', 'local_time', 'aqi', 'latitude', 'longitude']\n",
    "\n",
    "for feature in data.drop(drop,axis=1).columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(data[feature], bins=30, color='steelblue', edgecolor='black', density=True)\n",
    "    plt.title(f'Histogram of {feature}', fontsize=14)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYByPz5t2Ocy"
   },
   "source": [
    "### Result:\n",
    "This is the basic statistical analysis, because we have null values in our dataset and requires to fill them and we will handle it in `Data Cleaning Section`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNwwt-_g_jvt"
   },
   "source": [
    "**Resource For Skewness**:<br>\n",
    "[check here](https://www.quora.com/How-do-you-know-if-a-data-set-is-skewed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12vSkFiXWvqI"
   },
   "source": [
    "### Data Cleaning Section\n",
    "In this section we are going to clean the data and following bellow's steps:\n",
    "* Fill Kabul local data missing values.\n",
    "* Check the distribution and fix them.\n",
    "* Checking Correlation and Covariance and other advance methods.\n",
    "* Handling Kabul's date format.\n",
    "* Data Transformation handling.\n",
    "* Checking & Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "Sm7nzbmp2-jK",
    "outputId": "603ef96f-637f-4496-ecb1-fa76883d7a74"
   },
   "outputs": [],
   "source": [
    "# As Afghanistan data has 2 main features less than openweather, and these two features are not so efficient in AQI. We will drop those in feature Engineering.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "mT5Ro7tCX333",
    "outputId": "29dacef2-653b-4dd5-9666-0c2e2a1e30c3"
   },
   "outputs": [],
   "source": [
    "# Check the null values of NO as dataframe\n",
    "data[data['no'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ZZ5UV-NsYSPH",
    "outputId": "0c6a860b-3d00-432f-b240-523bfb2d6268"
   },
   "outputs": [],
   "source": [
    "# Checking statistical of Afghanistan data only\n",
    "data[data['state'] == 'Afghanistan'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "MPcj5jI6Z6sf",
    "outputId": "2f4623a1-977a-49b6-ee8c-6222d2a13757"
   },
   "outputs": [],
   "source": [
    "# Checking statistical of Kabul local data\n",
    "data[data['state'] == 'Kabul'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h5g5QzDl6qP"
   },
   "source": [
    "### Few visualization:\n",
    "Lets have a small visualization of Kabul local data and Openweather data features KDE and time series plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvCSLw3z3IeJ"
   },
   "source": [
    "**Further Analysis:**<br>\n",
    "Before imputing or droping our missing data we need to analyze our data more and check the `NH3` and `NO` in time series. Because our dataset is a time series data and we need to check them before filling those features, and there are many methods to handle it such as `Liner Interpolation`, `KNN Imputation`, `FTLRI`, `Forward & Backward`... but we need an efficient one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VtXOdfu4TuGY",
    "outputId": "44fe3324-5f57-4f2f-c323-deb6f7c65707"
   },
   "outputs": [],
   "source": [
    "# Lets check KDE (Kernal Desnity Plot) these features\n",
    "def kde_plot(dataset):\n",
    "  # unwanted features\n",
    "  drop = ['latitude', 'longitude', 'aqi', 'local_time', 'state', 'country']\n",
    "\n",
    "  # Kabul local data & openweather Kabul data\n",
    "  kabul_kde = dataset[dataset['state'] == 'Kabul']\n",
    "  openweather_kde = dataset[dataset['state'] == 'Afghanistan']\n",
    "\n",
    "  # droping unwanted features\n",
    "  dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "  for feature in dataset:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # KDE for Kabul data\n",
    "    sns.kdeplot(kabul_kde[feature].dropna(), label=f'Kabul {feature}', fill=True, color='blue')\n",
    "\n",
    "    # KDE for OpenWeather data\n",
    "    sns.kdeplot(openweather_kde[feature].dropna(), label=f'OpenWeather {feature}', fill=True, color='green')\n",
    "\n",
    "    plt.title(f'KDE Plot for {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "kde_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyMBL2bCnzeq"
   },
   "source": [
    "### Kernel Density Estimation\n",
    "You can check the links bellow for further information about KDE & Correlation:<br>\n",
    "`KDE`: [here](https://towardsdatascience.com/kernel-density-estimation-explained-step-by-step-7cc5b5bc4517).<br>\n",
    "`Correlation`: [here](https://medium.com/@rajneeshjha9s/measures-of-correlation-d8cae057085a#:~:text=Correlation%20is%20used%20to%20describe,with%20relation%20to%20each%20other.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ODgbbC2jazcQ",
    "outputId": "1c3cb142-b06f-4d44-8f06-2beae4841010"
   },
   "outputs": [],
   "source": [
    "# checking the correlation and covariance\n",
    "\"\"\"\n",
    "  Reminder:\n",
    "  This is just checking data gathered from Openweather Afghanistan, Kabul region\n",
    "  and Kabul officially gathered data from Government. This is not comparison between\n",
    "  all openweather data and Kabul local data.\n",
    "\"\"\"\n",
    "def corr_and_cov(dataset):\n",
    "  drop = ['latitude', 'longitude', 'aqi', 'local_time', 'state', 'country']\n",
    "  kabul_corr = data[data['state'] == 'Kabul'].drop(drop, axis=1).corr() # Kabul local data correlation\n",
    "  kabul_cov = data[data['state'] == 'Kabul'].drop(drop, axis=1).cov() # Kabul local data covariance\n",
    "\n",
    "  openweather_corr = data[data['state'] == 'Afghanistan'].drop(drop, axis=1).corr() # Openweather Kabul data correlation\n",
    "  openweather_cov = data[data['state'] == 'Afghanistan'].drop(drop, axis=1).cov() # Openweather Kabul data correlation\n",
    "\n",
    "  # Visualize correlation\n",
    "  plt.figure(figsize=(16, 12))\n",
    "\n",
    "  plt.subplot(2, 2, 1)\n",
    "  sns.heatmap(kabul_corr, annot=True, cmap='coolwarm', cbar=True, square=True)\n",
    "  plt.title('Kabul Local Data Correlation')\n",
    "\n",
    "  plt.subplot(2, 2, 2)\n",
    "  sns.heatmap(openweather_corr, annot=True, cmap='coolwarm', cbar=True, square=True)\n",
    "  plt.title('OpenWeather Kabul Data Correlation')\n",
    "\n",
    "  # Visualize covariance\n",
    "  plt.subplot(2, 2, 3)\n",
    "  sns.heatmap(kabul_cov, annot=True, cmap='viridis', cbar=True, square=True)\n",
    "  plt.title('Kabul Local Data Covariance')\n",
    "\n",
    "  plt.subplot(2, 2, 4)\n",
    "  sns.heatmap(openweather_cov, annot=True, cmap='viridis', cbar=True, square=True)\n",
    "  plt.title('OpenWeather Kabul Data Covariance')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "corr_and_cov(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB9YYXs2Vfx8"
   },
   "source": [
    "### Result:\n",
    "As you can see the `NO` and `NH3` is so correlated to other factors and this showes that we are going to **drop** these features in Feature Engineering section. But we can check the Temporal trend and after that lets see what will happen.<br>\n",
    "**Idea behind droping:**<br>\n",
    "`(NO)` & `(NH3)` are highly correlated with other pollutants such as `(CO)`, `(NO2)`, and `(SO2)`. Multicollinearity occurs when two or more predictors in a model are highly correlated, meaning they provide redundant information. In this case, \"no\" and \"nh3\" are strongly related to other features that are already in the model, leading to redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "KK5PSkl9qSX0",
    "outputId": "7f034bd9-5c0f-475d-852b-fc28777f1427"
   },
   "outputs": [],
   "source": [
    "# Drop unwantted features\n",
    "drop = ['latitude', 'longitude', 'country', 'state', 'aqi']\n",
    "openweather_data = data.drop(drop, axis=1)\n",
    "\n",
    "# Visualize nh3 over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(openweather_data.index, openweather_data['nh3'], label='NH3', color='blue')\n",
    "plt.plot(openweather_data.index, openweather_data['no'], label='NO', color='green')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Temporal Trends of NH3 and NO in OpenWeather Kabul Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "HsqFe4J-sV9F",
    "outputId": "bf007894-47f5-430a-858e-c0dbca10085d"
   },
   "outputs": [],
   "source": [
    "# Rolling mean for trend smoothing\n",
    "openweather_data['nh3_smoothed'] = openweather_data['nh3'].rolling(window=100).mean()\n",
    "openweather_data['no_smoothed'] = openweather_data['no'].rolling(window=100).mean()\n",
    "\n",
    "# Plot smoothed trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(openweather_data['nh3_smoothed'], label='Smoothed NH3', color='blue')\n",
    "plt.plot(openweather_data['no_smoothed'], label='Smoothed NO', color='green')\n",
    "plt.title('Mean Smoothed Trends of NH3 and NO')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0bx0Vn90ixX"
   },
   "source": [
    "`Brief Info of Temporal Trend Analysis`:<br>\n",
    "Temporal trends analysis involves examining and modeling how data points change over time, which is fundamental in time series analysis.`Resource` [here](https://falconediting.com/en/blog/time-series-analysis-understanding-temporal-trends-and-patterns)<br>\n",
    "`Brief Info of Rolling Mean Analysis`:<br>\n",
    "A rolling average, also known as a moving average, is a statistical method used to analyze data points by creating averages of different subsets of the complete dataset. This technique is commonly used in time series analysis to smooth out short-term fluctuations and highlight longer-term trends or cycles. `Resource`: [here](https://www.quora.com/What-is-rolling-average).<br>\n",
    "**Result:**<br>\n",
    "Based on Check up as we have done, now we can impute our missing data using `FTLRI imputation` methods. If it does not work we will drop these features<br>\n",
    "`Brief Info of FTLRI`: FTLRI is an effective time series air quality data imputation model that not only considers correlation, both in terms of time and attributes of the data points, but also legitimately utilizes logistic regression to deal with such correlation. `Resource` [here](https://www.mdpi.com/2073-4433/13/7/1044#:~:text=FTLRI%20is%20an%20effective%20time,to%20deal%20with%20such%20correlation). <br>\n",
    "The results show that FTLRI has a significant advantage over the compared imputation approaches, both in the particular short-term and long-term time series air quality data. Furthermore, FTLRI has good performance on datasets with a relatively high missing rate, since it only selects the data extremely related to the missing values instead of relying on all the other data like other methods. <br>\n",
    "`Resource`: Research Paper by Mei Chen, ORCID,Hongyu Zhu,Yongxu Chen and Youshuai Wang. <br>\n",
    " Click here for check up: [here](https://www.mdpi.com/2073-4433/13/7/1044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "2PfUhxGwiDXO",
    "outputId": "164d2ad4-d446-441f-fd7c-fb28b56b8de6"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\"\"\"\n",
    "To fill the missing values using FTLRI method we are going to classify our tasks into substeps\n",
    "in the following steps we first distinguash the data into two parts first the data gathered from openweather API\n",
    "and select Afghanistan records and the second one is Kabul Local data (as it has null values of no and nh3)\n",
    "the we are going to train our logistic model on openweather Kabul data and then fill the missing data of\n",
    "Kabul local data.\n",
    "\"\"\"\n",
    "# using a copy of the data\n",
    "temp = data.copy()\n",
    "\n",
    "# Step 1: Separate the dataset into training and target subsets\n",
    "afghanistan_data = temp[(temp['state'] == 'Afghanistan') & temp[['no', 'nh3']].notnull().all(axis=1)]\n",
    "kabul_data = temp[(temp['state'] == 'Kabul') & temp[['no', 'nh3']].isnull().any(axis=1)]\n",
    "\n",
    "# Features used for prediction (excluding 'no' and 'nh3' themselves to prevent leakage)\n",
    "features = [col for col in temp.columns if col not in ['no', 'nh3', 'state','longitude', 'latitude', 'country', 'local_time', 'aqi']]\n",
    "\n",
    "# Step 2: Train a Linear Regression model for 'no' and 'nh3' using Afghanistan data\n",
    "models = {}\n",
    "for target in ['no', 'nh3']:\n",
    "    X_train = afghanistan_data[features].dropna()\n",
    "    y_train = afghanistan_data.loc[X_train.index, target]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    models[target] = model\n",
    "\n",
    "# Step 3: Predict missing values for 'no' and 'nh3' in Kabul data\n",
    "for target in ['no', 'nh3']:\n",
    "    X_test = kabul_data[features].fillna(kabul_data[features].mean())\n",
    "    predictions = models[target].predict(X_test)\n",
    "    data.loc[X_test.index, target] = predictions\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RO-6KEtlQTm9",
    "outputId": "f41d85cb-6f9c-4e68-83b2-ba0b0fbad1f2"
   },
   "outputs": [],
   "source": [
    "# checking the correlation and covariance\n",
    "\"\"\"\n",
    "  Reminder:\n",
    "  This is just checking data gathered from Openweather Afghanistan, Kabul region\n",
    "  and Kabul officially gathered data from Government. This is not comparison between\n",
    "  all openweather data and Kabul local data.\n",
    "\"\"\"\n",
    "def corr_and_cov(dataset):\n",
    "  drop = ['latitude', 'longitude', 'aqi', 'local_time', 'state', 'country']\n",
    "  kabul_corr = data[data['state'] == 'Kabul'].drop(drop, axis=1).corr() # Kabul local data correlation\n",
    "  kabul_cov = data[data['state'] == 'Kabul'].drop(drop, axis=1).cov() # Kabul local data covariance\n",
    "\n",
    "  openweather_corr = data[data['state'] == 'Afghanistan'].drop(drop, axis=1).corr() # Openweather Kabul data correlation\n",
    "  openweather_cov = data[data['state'] == 'Afghanistan'].drop(drop, axis=1).cov() # Openweather Kabul data correlation\n",
    "\n",
    "  # Visualize correlation\n",
    "  plt.figure(figsize=(16, 12))\n",
    "\n",
    "  plt.subplot(2, 2, 1)\n",
    "  sns.heatmap(kabul_corr, annot=True, cmap='coolwarm', cbar=True, square=True)\n",
    "  plt.title('Kabul Local Data Correlation')\n",
    "\n",
    "  plt.subplot(2, 2, 2)\n",
    "  sns.heatmap(openweather_corr, annot=True, cmap='coolwarm', cbar=True, square=True)\n",
    "  plt.title('OpenWeather Kabul Data Correlation')\n",
    "\n",
    "  # Visualize covariance\n",
    "  plt.subplot(2, 2, 3)\n",
    "  sns.heatmap(kabul_cov, annot=True, cmap='viridis', cbar=True, square=True)\n",
    "  plt.title('Kabul Local Data Covariance')\n",
    "\n",
    "  plt.subplot(2, 2, 4)\n",
    "  sns.heatmap(openweather_cov, annot=True, cmap='viridis', cbar=True, square=True)\n",
    "  plt.title('OpenWeather Kabul Data Covariance')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "corr_and_cov(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "uZ5Qn2W4VveU",
    "outputId": "d5ee4532-fe6d-4723-dd0f-af767a5ed02f"
   },
   "outputs": [],
   "source": [
    "# checking kabul aqi\n",
    "data[data['state'] == 'Kabul']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qC2VOEaM-zZ"
   },
   "source": [
    "### Next step:\n",
    "In this step we are going to fill Afghanistan local data aqi with the formula using by India and USA.<br>\n",
    "AQI computing Formula: <br>\n",
    "$$\n",
    "I = \\frac{(I_{\\text{high}} - I_{\\text{low}})}{(C_{\\text{high}} - C_{\\text{low}})} \\times (C - C_{\\text{low}}) + I_{\\text{low}}\n",
    "$$\n",
    "\n",
    "\n",
    "where:<br>\n",
    "I = the Air Quality index,<br>\n",
    "C = the pollutant concentration,<br>\n",
    "Clow= the concentration breakpoint that is ≤C,<br>\n",
    "Chigh= the concentration breakpoint that is ≥C,<br>\n",
    "Ilow= the index breakpoint corresponding to Clow,<br>\n",
    "Ihigh= the index breakpoint corresponding to Chigh.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbk9qC5GUGaP"
   },
   "outputs": [],
   "source": [
    "# filling Afghanistan local aqi using USA formula\n",
    "# Define breakpoints for pollutants\n",
    "breakpoints = {\n",
    "    \"so2\": [(0, 19), (20, 79), (80, 249), (250, 349), (350, float('inf'))],\n",
    "    \"no2\": [(0, 39), (40, 69), (70, 149), (150, 199), (200, float('inf'))],\n",
    "    \"pm10\": [(0, 19), (20, 49), (50, 99), (100, 199), (200, float('inf'))],\n",
    "    \"pm2_5\": [(0, 9), (10, 24), (25, 49), (50, 74), (75, float('inf'))],\n",
    "    \"o3\": [(0, 59), (60, 99), (100, 139), (140, 179), (180, float('inf'))],\n",
    "    \"co\": [(0, 4399), (4400, 9399), (9400, 12399), (12400, 15399), (15400, float('inf'))]\n",
    "}\n",
    "\n",
    "# USA Index ranges for AQI (qualitative classification ranges)\n",
    "aqi_indices = [(0, 50), (51, 100), (101, 150), (151, 200), (201, 300)]\n",
    "\n",
    "# calculate AQI for a pollutant\n",
    "def calculate_aqi(concentration, pollutant):\n",
    "    for (C_lo, C_hi), (I_lo, I_hi) in zip(breakpoints[pollutant], aqi_indices):\n",
    "        if C_lo <= concentration < C_hi:\n",
    "            return ((I_hi - I_lo) / (C_hi - C_lo)) * (concentration - C_lo) + I_lo\n",
    "    return None  # if data is out of bound\n",
    "\n",
    "# Map AQI values to their qualitative classification (1, 2, 3, 4, 5)\n",
    "def map_aqi_to_classification(aqi):\n",
    "    if aqi < 51:\n",
    "        return 1  # Good\n",
    "    elif aqi < 101:\n",
    "        return 2  # Fair\n",
    "    elif aqi < 151:\n",
    "        return 3  # Moderate\n",
    "    elif aqi < 201:\n",
    "        return 4  # Abnormal\n",
    "    else:\n",
    "        return 5  # Dangerous\n",
    "\n",
    "# Fill missing AQI values based on the calculated AQI from pollutants\n",
    "def fill_aqi(row):\n",
    "    if pd.isnull(row[\"aqi\"]):\n",
    "        aqi_values = []\n",
    "        for pollutant in breakpoints.keys():\n",
    "            if not pd.isnull(row[pollutant]):\n",
    "                aqi = calculate_aqi(row[pollutant], pollutant)\n",
    "                if aqi is not None:\n",
    "                    aqi_values.append(aqi)\n",
    "        # If we have valid AQI values, choose the maximum\n",
    "        if aqi_values:\n",
    "            max_aqi = max(aqi_values)\n",
    "            return map_aqi_to_classification(max_aqi)  # Map to classification index\n",
    "        else:\n",
    "            return np.nan  # Return NaN if no valid AQI values found\n",
    "    return row[\"aqi\"]\n",
    "\n",
    "\n",
    "copy = data.copy()\n",
    "copy[\"aqi\"] = copy.apply(fill_aqi, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJklwi4maVRw",
    "outputId": "c449539b-9238-4297-f511-929c47f39d6f"
   },
   "outputs": [],
   "source": [
    "# checking does we have null values in aqi feature\n",
    "copy['aqi'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "vx72aC5easC4",
    "outputId": "c8bb5bd4-7dab-4f40-cc00-1344c5e65aca"
   },
   "outputs": [],
   "source": [
    "# checking total null values\n",
    "copy.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvR5e7KCeLOx"
   },
   "source": [
    "### Relationship Analysis:\n",
    "In these few cells we are going to check:\n",
    "* Correlation\n",
    "* Covarience\n",
    "* MI(\tMutual Information )\n",
    "\n",
    "#### Note:\n",
    "We are checking the relationship of features with target column using MI but take note that we first are not considering (Amonia) and (Nitrogen monoxide) for the first time but the second plot we will, and it will help us for furthor decision making. And we will handle this on Feature Engineering. But we want to familiarize ourself with data in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "BmxGBYu3bHge",
    "outputId": "55e60c55-a141-4302-f967-5c3219a7da10"
   },
   "outputs": [],
   "source": [
    "# correlation and covarience with no & nh3\n",
    "def corr_and_cov(dataset, drop_columns):\n",
    "\n",
    "    filtered_data = dataset.drop(columns=drop_columns, axis=1)\n",
    "\n",
    "    # Compute correlation and covariance\n",
    "    corr = filtered_data.corr()  # Correlation matrix\n",
    "    cov = filtered_data.cov()   # Covariance matrix\n",
    "\n",
    "    # Dynamically adjust figsize based on feature count\n",
    "    num_features = len(filtered_data.columns)\n",
    "    figsize = (num_features * 2, num_features * 0.8)  # Adjust scaling factors as needed\n",
    "\n",
    "    # Create subplots\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Correlation heatmap\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', cbar=True, annot_kws={\"size\": 8}, fmt=\".2f\")\n",
    "    plt.title('Data Correlation', fontsize=14)\n",
    "\n",
    "    # Covariance heatmap\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(cov, annot=True, cmap='viridis', cbar=True, annot_kws={\"size\": 8}, fmt=\".2f\")\n",
    "    plt.title('Data Covariance', fontsize=14)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Drop irrelevant features\n",
    "drop_columns = ['latitude', 'longitude', 'local_time', 'state', 'country']\n",
    "corr_and_cov(copy, drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "id": "AF4PPlPWwHTH",
    "outputId": "1a10c94d-6bc8-4ba6-f7ed-ced2bb98adc2"
   },
   "outputs": [],
   "source": [
    "# correlation and covarience without no & nh3\n",
    "drop_columns = ['latitude', 'longitude', 'local_time', 'state', 'country', 'no', 'nh3']\n",
    "corr_and_cov(copy, drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rJ2EY3AwfMz"
   },
   "source": [
    "Lets go further analysis with MI or mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "GxXeM5OibHeF",
    "outputId": "b232a256-8d00-4375-c11b-8dd42a3b8989"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "# Mutual Information classification without (NH3, NO)\n",
    "\n",
    "def calculate_mutual_information(dataset, target, task_type, drop_columns, bins=None, labels=None):\n",
    "    \"\"\"\n",
    "    Calculate and visualize Mutual Information for dataset features.\n",
    "    \"\"\"\n",
    "    # Drop unnecessary columns\n",
    "    dataset = dataset.drop(drop_columns, axis=1)\n",
    "\n",
    "    # Discretize target for classification tasks if bins are provided\n",
    "    if task_type == 'classification' and bins is not None and labels is not None:\n",
    "        target = pd.cut(target, bins=bins, labels=labels)\n",
    "\n",
    "    # Choose mutual information function based on task type\n",
    "    if task_type == 'classification':\n",
    "        mi = mutual_info_classif(dataset, target, random_state=42)\n",
    "    elif task_type == 'regression':\n",
    "        mi = mutual_info_regression(dataset, target, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_type. Use 'classification' or 'regression'.\")\n",
    "\n",
    "    # Create a Pandas Series of MI scores\n",
    "    mi_scores = pd.Series(mi, index=dataset.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    # Plot Mutual Information Scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mi_scores.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Mutual Information Scores', fontsize=14)\n",
    "    plt.ylabel('Mutual Information', fontsize=12)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "drop_columns = ['latitude', 'longitude', 'local_time', 'state', 'country', 'no', 'nh3']\n",
    "\n",
    "# For classification\n",
    "bins = [0, 50, 100, 150, 200, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "calculate_mutual_information(copy.drop('aqi', axis=1), copy['aqi'], 'classification', drop_columns, bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "EKd_UFJ2bHbK",
    "outputId": "2df72907-abba-4f53-f5d9-843599ad0f77"
   },
   "outputs": [],
   "source": [
    "# Regression mutual information without (NO, NH3).\n",
    "calculate_mutual_information(copy.drop('aqi', axis=1), copy['aqi'], 'regression', drop_columns, bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "96rdHRQ6Clfz",
    "outputId": "23bd877d-08c8-4288-bfc0-3c71f1eca6d2"
   },
   "outputs": [],
   "source": [
    "# MI Classification with (NO, NH3)\n",
    "drop_columns = ['latitude', 'longitude', 'local_time', 'state', 'country']\n",
    "\n",
    "# For classification\n",
    "bins = [0, 50, 100, 150, 200, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "calculate_mutual_information(copy.drop('aqi', axis=1), copy['aqi'], 'classification', drop_columns, bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "vNIdr1Q2Clce",
    "outputId": "696fa57b-d81c-46f2-a8eb-bee3a041f539"
   },
   "outputs": [],
   "source": [
    "# MI Regression with (NO, NH3)\n",
    "calculate_mutual_information(copy.drop('aqi', axis=1), copy['aqi'], 'regression', drop_columns, bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RINYChhi0TPJ"
   },
   "source": [
    "### Result:\n",
    "The correlation and MI showes poor relationship with each features and we will use it again in `Feature Engineering` too. There are many steps remain to we make a decision about poor relationship right now just be patience and drink a cap of coffee or tea☕.<br>\n",
    "Please be patience and continue the journey with us. 😉😊😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtPTMMlpkKR5"
   },
   "source": [
    "### Handling Duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFcDIjvIkJtv",
    "outputId": "c2672525-5b17-40ab-b594-d9cdb1ce0530"
   },
   "outputs": [],
   "source": [
    "# again checking for duplication\n",
    "copy.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "cGCe7sFLNGyh",
    "outputId": "3659d464-f038-4b00-fb84-043b506601a4"
   },
   "outputs": [],
   "source": [
    "# checking duplicate records\n",
    "copy[copy.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXX3GQUtNGvR",
    "outputId": "df42f18e-25c7-4020-ccf8-19d5e397944d"
   },
   "outputs": [],
   "source": [
    "# checking duplicate without some features\n",
    "drop = ['country', 'state', 'aqi', 'local_time', 'latitude', 'longitude']\n",
    "copy.drop(drop, axis=1).duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "PJN9jjmcNwJ_",
    "outputId": "2c2c7c98-5626-48e5-f6a2-95a290f21fb8"
   },
   "outputs": [],
   "source": [
    "# checking duplicate as dataframe\n",
    "copy[copy.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6E0klcRNwCj",
    "outputId": "d5cc67f6-320c-4c0b-d796-ee19129cc541"
   },
   "outputs": [],
   "source": [
    "# We need further analysis of duplication using in every feature\n",
    "# To do this we will create a function named duplicate and process the duplication\n",
    "def duplicate(dataset):\n",
    "  duplication = {} # saving the duplicated as key is the name of feature and value is amount of duplication\n",
    "\n",
    "  for feature in dataset.columns:\n",
    "    duplication[feature] = dataset.duplicated(subset=[feature]).count()\n",
    "\n",
    "  return duplication\n",
    "\n",
    "# let's check now\n",
    "res = duplicate(copy)\n",
    "for key, val in res.items():\n",
    "  print(f\"Total {key} duplicate values: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "rjvWnKRIRDB4",
    "outputId": "a0767796-f94b-4f1d-91c1-d1e6cf2072a4"
   },
   "outputs": [],
   "source": [
    "copy.duplicated(subset=['co'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8nzRg_QNXqoY",
    "outputId": "ec02cd2d-5182-4d89-aa1a-7448595069a2"
   },
   "outputs": [],
   "source": [
    "du = copy[copy.duplicated(keep=False)]\n",
    "du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "Em17l0oN-FPl",
    "outputId": "29400843-d4d4-4a75-e460-450c31ea4c01"
   },
   "outputs": [],
   "source": [
    "dup = copy.duplicated()\n",
    "dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJj-1n3Z-FMJ",
    "outputId": "bc55c28e-6ed7-4047-b366-7a50566f7a5a"
   },
   "outputs": [],
   "source": [
    "# checking only duplicate index\n",
    "# this process is to make sure myself\n",
    "def duplicate(dataset):\n",
    "  dic = {}\n",
    "  count = 0\n",
    "  for index, du in enumerate(dataset):\n",
    "    if du == True:\n",
    "      count += 1\n",
    "      dic[index] = du\n",
    "\n",
    "  return dic, count\n",
    "\n",
    "res, total = duplicate(dup)\n",
    "\n",
    "print(res)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "uMDASeTf-FJx",
    "outputId": "21e2c641-2e37-43e1-c581-5fe0ba0a1ada"
   },
   "outputs": [],
   "source": [
    "copy[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roXo8-hpGe2K"
   },
   "outputs": [],
   "source": [
    "# time to remove duplicate records\n",
    "copy.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "WNMKboRMNgJD",
    "outputId": "8ab0f1ef-cc2d-44e0-b349-e5433cf40ada"
   },
   "outputs": [],
   "source": [
    "# checking the duplicatin to make sure everything goes smoothly\n",
    "copy[data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koFHdO9ZiyYQ"
   },
   "source": [
    "#### Distribution Checkup:\n",
    "As you know we have checked some distribution checkup previously to fimilarize ourself with the data. But here we are going to check:\n",
    "* Normal Distribution.\n",
    "* Density.\n",
    "* Statistical Analysis with Density.\n",
    "* Shapiro Wilk.\n",
    "* QQ Plot for Normalization Checkup.\n",
    "* Log-Normal, Exponential and Gamma in one plot.\n",
    "\n",
    "Hope to Enjoy 😎😎😎."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "McqwuT4VyTwq",
    "outputId": "2420cea2-4777-4bff-938a-81b4a529bdf9"
   },
   "outputs": [],
   "source": [
    "# Checking distribution of data\n",
    "def normal_dis(dataset):\n",
    "  # Drop unwanted features\n",
    "  drop = ['country', 'state', 'aqi', 'latitude', 'longitude', 'local_time']\n",
    "  dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "  # visualizing each feature using PDF\n",
    "  for feature in dataset.columns:\n",
    "    fig = px.histogram(dataset[feature], x=f\"{feature}\", title=f\"{feature} Distribution\")\n",
    "    fig.show()\n",
    "  return\n",
    "normal_dis(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8IZbXW6zmiu-",
    "outputId": "c7d4ff73-591d-4f57-fbad-110b6c618c35"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from scipy.stats import norm\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "# Dropping unnecessary columns if any\n",
    "normal_feature = copy.drop(['latitude', 'longitude', 'country', 'state', 'local_time', 'aqi'], axis=1, errors='ignore')\n",
    "\n",
    "# Loop through features to plot statistical analysis using Plotly\n",
    "for feature in normal_feature.columns:\n",
    "    feature_data = normal_feature[feature].dropna()\n",
    "\n",
    "    # Calculate key statistics\n",
    "    mean = feature_data.mean()\n",
    "    median = feature_data.median()\n",
    "    std = feature_data.std()\n",
    "    variance = feature_data.var()\n",
    "\n",
    "    # Create the histogram and KDE plot with Plotly\n",
    "    hist_data = go.Histogram(\n",
    "        x=feature_data,\n",
    "        histnorm='density',\n",
    "        name='Histogram',\n",
    "        opacity=0.7,\n",
    "        marker=dict(color='steelblue'),\n",
    "        nbinsx=30\n",
    "    )\n",
    "\n",
    "    # Calculate the normal distribution curve\n",
    "    x_range = np.linspace(min(feature_data), max(feature_data), 100)\n",
    "    y_range = norm.pdf(x_range, mean, std)\n",
    "\n",
    "    normal_curve = go.Scatter(\n",
    "        x=x_range,\n",
    "        y=y_range,\n",
    "        mode='lines',\n",
    "        name='Normal Distribution',\n",
    "        line=dict(color='purple', width=2)\n",
    "    )\n",
    "\n",
    "    # Add vertical lines for Mean, Median, and +/- 1 Standard Deviation\n",
    "    mean_line = go.Scatter(\n",
    "        x=[mean, mean],\n",
    "        y=[0, max(y_range) * 1.1],  # Extend the line along the y-axis\n",
    "        mode='lines',\n",
    "        name=f'Mean = {mean:.2f}',\n",
    "        line=dict(color='red', dash='dash', width=3)\n",
    "    )\n",
    "\n",
    "    median_line = go.Scatter(\n",
    "        x=[median, median],\n",
    "        y=[0, max(y_range) * 1.1],  # Extend the line along the y-axis\n",
    "        mode='lines',\n",
    "        name=f'Median = {median:.2f}',\n",
    "        line=dict(color='green', dash='dash', width=3)\n",
    "    )\n",
    "\n",
    "    std_plus_line = go.Scatter(\n",
    "        x=[mean + std, mean + std],\n",
    "        y=[0, max(y_range) * 1.1],  # Extend the line along the y-axis\n",
    "        mode='lines',\n",
    "        name=f'+1 SD = {mean + std:.2f}',\n",
    "        line=dict(color='orange', dash='dash', width=3)\n",
    "    )\n",
    "\n",
    "    std_minus_line = go.Scatter(\n",
    "        x=[mean - std, mean - std],\n",
    "        y=[0, max(y_range) * 1.1],  # Extend the line along the y-axis\n",
    "        mode='lines',\n",
    "        name=f'-1 SD = {mean - std:.2f}',\n",
    "        line=dict(color='orange', dash='dash', width=3)\n",
    "    )\n",
    "\n",
    "    # Add annotations for variance\n",
    "    variance_text = go.layout.Annotation(\n",
    "        x=mean + 3 * std,\n",
    "        y=max(y_range) * 0.05,\n",
    "        text=f'Variance = {variance:.2f}',\n",
    "        showarrow=True,\n",
    "        font=dict(size=12, color='purple'),\n",
    "        align='left'\n",
    "    )\n",
    "\n",
    "    # Create the layout with static range\n",
    "    layout = go.Layout(\n",
    "        title=f'Statistical Analysis of {feature}',\n",
    "        xaxis=dict(\n",
    "            title=feature,\n",
    "            showgrid=True,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Density',\n",
    "            rangemode='tozero',  # Ensure the y-axis starts from zero\n",
    "            showgrid=True,\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        template=\"plotly_dark\",  # Optional, change to \"plotly\" for a lighter theme\n",
    "        annotations=[variance_text]\n",
    "    )\n",
    "\n",
    "    # Create the figure and show\n",
    "    fig = go.Figure(\n",
    "        data=[hist_data, normal_curve, mean_line, median_line, std_plus_line, std_minus_line],\n",
    "        layout=layout\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9OkEPieJpJXG",
    "outputId": "188cd55f-8673-46e2-a2ea-df26852def0c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "normal_feature = copy.drop(['latitude', 'longitude', 'country', 'state', 'local_time', 'aqi'], axis=1, errors='ignore')\n",
    "\n",
    "# Loop through features to plot statistical analysis using Seaborn and Matplotlib\n",
    "for feature in normal_feature.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Get feature data and drop null values\n",
    "    feature_data = normal_feature[feature].dropna()\n",
    "\n",
    "    # Calculate key statistics\n",
    "    mean = feature_data.mean()\n",
    "    median = feature_data.median()\n",
    "    std = feature_data.std()\n",
    "    variance = feature_data.var()\n",
    "\n",
    "    # Plot the density curve with Seaborn\n",
    "    sns.histplot(feature_data, kde=True, color='steelblue', stat='density', label='Density Curve')\n",
    "\n",
    "    # Plot mean, median, and standard deviation lines\n",
    "    plt.axvline(mean, color='red', linestyle='dashed', linewidth=3, label=f'Mean = {mean:.2f}')\n",
    "    plt.axvline(median, color='green', linestyle='dashed', linewidth=3, label=f'Median = {median:.2f}')\n",
    "    plt.axvline(mean + std, color='orange', linestyle='dashed', linewidth=3, label=f'+1 SD = {mean + std:.2f}')\n",
    "    plt.axvline(mean - std, color='orange', linestyle='dashed', linewidth=3, label=f'-1 SD = {mean - std:.2f}')\n",
    "\n",
    "\n",
    "    # Title and labels\n",
    "    plt.title(f'Statistical Analysis of {feature}', fontsize=16)\n",
    "    plt.xlabel(feature, fontsize=14)\n",
    "    plt.ylabel('Density', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugTuTSBk_ig_"
   },
   "source": [
    "Probability Density Estimation mostly showes un-normal distribution but still we want to make sure about it and check Shapiro Milk method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjfzHDUVvkHd",
    "outputId": "b3a1e6f9-4a8b-4180-e95c-f0f0f8b26fc7"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "def shapiro_test(dataset):\n",
    "  # Perform the Shapiro-Wilk and saving each feature result in results\n",
    "  results = {}\n",
    "\n",
    "  # drop unwanted feature\n",
    "  drop = ['aqi', 'local_time','longitude', 'latitude', 'country', 'state']\n",
    "  dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "  for feature in dataset.columns:\n",
    "      feature_data = dataset[feature].dropna()  # Drop missing values\n",
    "      stat, p_value = shapiro(feature_data)\n",
    "      results[feature] = {'Statistic': stat, 'P-Value': p_value}\n",
    "\n",
    "\n",
    "  # Interpretation\n",
    "  for feature, result in results.items():\n",
    "    p_value = result['P-Value']\n",
    "    if p_value > 0.05:\n",
    "        print(f\"{feature}: Likely Normally Distributed (p = {p_value:.4f})\")\n",
    "    else:\n",
    "        print(f\"{feature}: Not Normally Distributed (p = {p_value:.4f})\")\n",
    "\n",
    "shapiro_test(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nHD08_9BUG0",
    "outputId": "4a69f716-82c2-4fac-9577-6f68f60b325e"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kstest, norm\n",
    "\n",
    "def Kolmogorov_Smirnov(dataset):\n",
    "  # Perform the K-S test for each feature\n",
    "  results = {}\n",
    "\n",
    "  # drop unwanted feature\n",
    "  drop = ['aqi', 'local_time','longitude', 'latitude', 'country', 'state']\n",
    "  dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "\n",
    "  for feature in dataset.columns:\n",
    "      feature_data = dataset[feature].dropna()  # Drop null values\n",
    "      # Standardize the data (mean=0, std=1) for comparison with normal distribution\n",
    "      standardized_data = (feature_data - feature_data.mean()) / feature_data.std()\n",
    "\n",
    "      # Perform K-S test against the standard normal distribution\n",
    "      stat, p_value = kstest(standardized_data, 'norm')\n",
    "      results[feature] = {'Statistic': stat, 'P-Value': p_value}\n",
    "\n",
    "\n",
    "  # Interpretation\n",
    "  for feature, result in results.items():\n",
    "      p_value = result['P-Value']\n",
    "      if p_value > 0.05:\n",
    "        print(f\"{feature}: Likely Normally Distributed (p = {p_value:.4f})\")\n",
    "      else:\n",
    "        print(f\"{feature}: Not Normally Distributed (p = {p_value:.4f})\")\n",
    "\n",
    "Kolmogorov_Smirnov(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jUQtyOuXOE2J",
    "outputId": "2c6b38c7-3c5b-4b4d-8312-6ca5c9b99bc9"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def qq_plot(dataset):\n",
    "  # drop unwanted features\n",
    "  drop = ['aqi', 'local_time','longitude', 'latitude', 'country', 'state']\n",
    "  dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "  for feature in dataset.columns:\n",
    "      plt.figure(figsize=(10, 6))\n",
    "\n",
    "      # Q-Q Plot\n",
    "      (osm, osr), (slope, intercept, r) = stats.probplot(data[feature], dist=\"norm\", plot=None)\n",
    "      plt.scatter(osm, osr, color='steelblue', edgecolor='black', alpha=0.7, label=\"Data Points\")\n",
    "      plt.plot(osm, slope * np.array(osm) + intercept, color='red', lw=2, label=f\"Fit Line (R² = {r**2:.2f})\")\n",
    "\n",
    "      # Annotations for key regions\n",
    "      plt.axvline(0, color='gray', linestyle='--', alpha=0.7)  # Mean\n",
    "      plt.axhline(0, color='gray', linestyle='--', alpha=0.7)  # Theoretical mean\n",
    "\n",
    "      # Add legend\n",
    "      plt.legend(fontsize=12)\n",
    "\n",
    "      # Labels and Title\n",
    "      plt.title(f\"Q-Q Plot for {feature}\", fontsize=16)\n",
    "      plt.xlabel(\"Theoretical Quantiles\", fontsize=14)\n",
    "      plt.ylabel(\"Sample Quantiles\", fontsize=14)\n",
    "      plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "      plt.show()\n",
    "\n",
    "qq_plot(copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9PldTquQThT"
   },
   "source": [
    "### Result\n",
    "Based on description above we are now sure that our data is not normal distributed and can check other distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JgKNBdERQHVF",
    "outputId": "893b41a6-b7f9-4cd1-c382-ed2825d2392f"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def overlaping_pdf(dataset):\n",
    "    # Drop unwanted features\n",
    "    drop = ['aqi', 'local_time', 'longitude', 'latitude', 'country', 'state']\n",
    "    dataset = dataset.drop(columns=drop, errors='ignore')\n",
    "\n",
    "    # Plot histogram of the data for each feature\n",
    "    for feature in dataset.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Get feature data\n",
    "        feature_data = dataset[feature].dropna()\n",
    "\n",
    "        # Plot histogram of the data\n",
    "        sns.histplot(feature_data, kde=False, stat='density', bins=30, color='steelblue', label='Data')\n",
    "\n",
    "        # Prepare x-axis values for PDF plotting\n",
    "        x = np.linspace(feature_data.min(), feature_data.max(), 1000)\n",
    "\n",
    "        # Distributions to fit\n",
    "        distributions = {\n",
    "            'Normal': stats.norm,\n",
    "            'Log-Normal': stats.lognorm,\n",
    "            'Exponential': stats.expon,\n",
    "            'Gamma': stats.gamma\n",
    "        }\n",
    "\n",
    "        # Fit and plot PDFs for different distributions\n",
    "        for name, dist in distributions.items():\n",
    "            try:\n",
    "                if name == 'Log-Normal':\n",
    "                    # Filter positive values for log-normal\n",
    "                    positive_data = feature_data[feature_data > 0]\n",
    "                    if len(positive_data) > 0:  # Ensure there are positive values\n",
    "                        params = dist.fit(positive_data)\n",
    "                        plt.plot(x, dist.pdf(x, *params), label=f'{name} (fit)', lw=2)\n",
    "                else:\n",
    "                    # Fit other distributions\n",
    "                    params = dist.fit(feature_data)\n",
    "                    plt.plot(x, dist.pdf(x, *params[:-2], loc=params[-2], scale=params[-1]), label=f'{name} (fit)', lw=2)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not fit {name} for {feature}: {e}\")\n",
    "\n",
    "        # Labels, title, and legend\n",
    "        plt.title(f'Comparing Distributions for {feature}', fontsize=16)\n",
    "        plt.xlabel('Value', fontsize=14)\n",
    "        plt.ylabel('Density', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "# Call the function\n",
    "overlaping_pdf(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfhrD0s0Q43U",
    "outputId": "b6f9aed4-cc33-4ec9-aedc-b65b1011b4e3"
   },
   "outputs": [],
   "source": [
    "# installing profiling\n",
    "# !pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "1fc0598fc4fc4f2fb1bcb4f1bac6427b",
      "1eff73fbf07b4b1394801938f9997c1a",
      "adcbdc5efbe644e38024c8de4e51d7d1",
      "985989c32a534ec1826f9f21751f8522",
      "417912ed8b4e4dc493269c5cf09f53d9",
      "3361aaeb7be74ba3823590282ee8ecf3",
      "f50909b3526240e5a34b582fa9542c60",
      "d6b11e3a8882432bb8d574f6436e2899",
      "b1369932757a4e93bbaf7cb24e4dfe63",
      "d26b25adf0644359b1ef7eec292d9390",
      "bf54d793da20428aa1455e648c59d42a",
      "5c53113273dd4db482d9a660ebb7fb37",
      "b45f2dc03095463e94300f1e0daf4224",
      "3d8b57fdcf074327996b702f7257ab9b",
      "3eb6de40a18445c69fc8f6c3fc211295",
      "c1be10dcf3ab4a28b42acf991a3f1fce",
      "881d809c358c4905b094c5baa7861510",
      "453f95a6c3284e5297b49c15137696df",
      "300ac91f3d144d9da521261290e3580a",
      "bd4d1d1f852e40beb5f5633d6994853f",
      "eb64a0ada3cf44789f7cb09e8f8baea6",
      "e10c935c79844ed7a394e0dfff6fd6bb",
      "32ab583530d74b338acf8e713edcc4c9",
      "d2e9a68fd6604a5d927c2e1b95cf698f",
      "f41693ca8ad949cf9760f3c13811295d",
      "88b65cf13ee04cc293309823b8588c66",
      "d21ad422038b48ce95d8f19f68434b6c",
      "c0aecd389f99479da98a4df0ed2e13f3",
      "aaa28a96dd394f529f082ab31b092240",
      "c0ca51e5a0134b499bf5fb5713d55eed",
      "3f2b388313b54be8ae152db0a680ab22",
      "226b7f00a1a34f56a7964043cf0b6630",
      "1468bdad0bb94b2f9326f9fd106ec371"
     ]
    },
    "id": "JB1QnZBXW214",
    "outputId": "947dbdfd-2a49-4966-8585-f398a58e86db"
   },
   "outputs": [],
   "source": [
    "# checking full data report using profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(copy, title='Dataset Report')\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ2L_OFnlzep"
   },
   "source": [
    "### Stationarity in Time Series:\n",
    "Before immersion in transformation, it is important that we have to check the stationarity of our features and based on that we will make a decision about transformation. <br>\n",
    "**Brief Info about Stationarity:**<br>\n",
    "Stationarity in time series refers to a property where the statistical characteristics of the series, such as mean, variance, and autocorrelation, remain constant over time. This means that the behavior of the series does not depend on the time at which it is observed, and its unconditional joint probability distribution does not change when shifted in time.<br>\n",
    "You can check about stationarity and non-stationarity in [here](https://medium.com/@ritusantra/stationarity-in-time-series-887eb42f62a9).<br>\n",
    "For further information please check [here](https://www.analyticsvidhya.com/blog/2021/06/statistical-tests-to-check-stationarity-in-time-series-part-1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MwkVFKBYlzLp",
    "outputId": "569602e4-4533-43ec-bc8a-423da1c096a0"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def check_stationarity(dataset, time_col):\n",
    "    # Ensure the time column is in datetime format\n",
    "    dataset[time_col] = pd.to_datetime(dataset[time_col], errors='coerce', dayfirst=True,  format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Set time column as index for time series operations\n",
    "    dataset = dataset.set_index(time_col)\n",
    "\n",
    "    # Iterate through each feature\n",
    "    for feature in dataset.columns:\n",
    "        print(f\"\\nAnalyzing Feature: {feature}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Plot the time series\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(dataset[feature], color='blue', label=f'{feature} Time Series')\n",
    "        plt.title(f'{feature} Over Time', fontsize=16)\n",
    "        plt.xlabel('Time', fontsize=14)\n",
    "        plt.ylabel(feature, fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "        # Perform the Augmented Dickey-Fuller (ADF) Test\n",
    "        print(\"Performing ADF Test...\")\n",
    "        result = adfuller(dataset[feature].dropna())  # Drop NaNs to avoid errors\n",
    "        print(f\"ADF Statistic: {result[0]}\")\n",
    "        print(f\"p-value: {result[1]}\")\n",
    "        print(f\"Critical Values: {result[4]}\")\n",
    "\n",
    "        # Check stationarity\n",
    "        if result[1] <= 0.05:\n",
    "            print(\"Result: The feature is stationary (p-value <= 0.05).\")\n",
    "        else:\n",
    "            print(\"Result: The feature is non-stationary (p-value > 0.05).\")\n",
    "\n",
    "# drop unwanted features\n",
    "drop = ['country', 'state', 'aqi', 'longitude', 'latitude']\n",
    "dataset = copy.drop(drop, axis=1)\n",
    "check_stationarity(dataset, 'local_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhjqTcSGsrrU"
   },
   "source": [
    "### Result:\n",
    "All the feature are stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np7rV2VZ1aPh"
   },
   "source": [
    "### Outliers Checkup:\n",
    "We are going to check outliers using boxplot and IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y9mwP22B1Z2B",
    "outputId": "75f70bd5-ea03-4f2f-c2b3-7537765c82c3"
   },
   "outputs": [],
   "source": [
    "def outliers(dataset):\n",
    "  # unwanted features\n",
    "  drop = ['country', 'state', 'longitude', 'latitude', 'aqi', 'local_time']\n",
    "  dataset = dataset.drop(drop, axis=1)\n",
    "  for feature in dataset.columns:\n",
    "    # Calculate Q1 (25th percentile), Q3 (75th percentile) and IQR (Interquartile Range)\n",
    "    Q1 = dataset[feature].quantile(0.25)\n",
    "    Q3 = dataset[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the lower and upper bounds for detecting outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Displaying lower and upper bounds\n",
    "    print(f\"Lower Bound: {lower_bound}\")\n",
    "    print(f\"Upper Bound: {upper_bound}\")\n",
    "\n",
    "    # Identifying outliers and normal values\n",
    "    dataset['outlier'] = np.where((dataset[feature] < lower_bound) | (dataset[feature] > upper_bound), 'Outlier', 'Normal')\n",
    "\n",
    "    # Create the interactive boxplot using Plotly\n",
    "    fig = px.box(dataset,\n",
    "                x=feature,\n",
    "                color=\"outlier\",\n",
    "                category_orders={\"outlier\": [\"Normal\", \"Outlier\"]},\n",
    "                labels={feature: f\"{feature} Value\", \"outlier\": \"Outlier Status\"},\n",
    "                title=\"Outlier Detection using Boxplot\",\n",
    "                points=\"all\"  # Show all points, including outliers\n",
    "                )\n",
    "\n",
    "    # Customize the plot to make it clearer\n",
    "    fig.update_traces(marker=dict(size=8),  # Increase point size for better visibility\n",
    "                      boxmean='sd')  # Show the mean as a line\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "outliers(copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8c3M2Yae2U7"
   },
   "source": [
    "### Transformation:\n",
    "Based on different types of distribution, Normal checkup distribution we have done. It showed that our data is not normal distributed, in addition, we have right or positive skew distribution that requires to be transformed but as we checked stationary, our dataset is stationary and does not requires transformation. But as our goal is to remove outliers, building a prediction and classification algorithm of neural network with the combination of reinforcement learning, we need to use transformation to robust the data.<br>\n",
    "`Different types of Transformation:`\n",
    "* Box cox transformation\n",
    "* Yeo-Johnson Transformation\n",
    "* log-normal Transformation\n",
    "* ...\n",
    "<br>Each of these transformation is used for specific purpose. but as we have knowledge of our dataset and we have zero, negative and positive values, right skewed and time series dataset. One of best option for us is using `Yeo-Johnson Transformation`.<br>\n",
    "please check for more details [here](https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FECDCXVZR4wy",
    "outputId": "48297b3f-9833-4069-df73-8daa5eaf8acc"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def power_transformation(dataset):\n",
    "    # Unwanted features to drop\n",
    "    drop = ['country', 'state', 'longitude', 'latitude', 'local_time', 'aqi']\n",
    "    dropped_features = dataset[drop]  # Save the dropped features to merge later\n",
    "    dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "    # Apply Yeo-Johnson transformation to each feature individually\n",
    "    transformed_features = {}\n",
    "    for feature in dataset.columns:\n",
    "        transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "        transformed_features[feature] = transformer.fit_transform(dataset[[feature]]).flatten()\n",
    "\n",
    "    # Create a new DataFrame with transformed features\n",
    "    transformed_data = pd.DataFrame(transformed_features)\n",
    "\n",
    "    # Join the dropped features back into the dataset\n",
    "    final_dataset = pd.concat([dropped_features.reset_index(drop=True), transformed_data], axis=1)\n",
    "\n",
    "    # Print some statistics to compare\n",
    "    for col in dataset.columns:\n",
    "        print(f\"Feature: {col}\")\n",
    "        print(f\" - Skewness before: {dataset[col].skew():.2f}\")\n",
    "        print(f\" - Skewness after: {transformed_data[col].skew():.2f}\")\n",
    "        print(f\" - Mean before: {dataset[col].mean():.2f}\")\n",
    "        print(f\" - Mean after: {transformed_data[col].mean():.2f}\")\n",
    "        print()\n",
    "\n",
    "    return final_dataset  # Return the transformed dataset\n",
    "\n",
    "transformed_data = power_transformation(copy)\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IJH1OJRcU_a7",
    "outputId": "27a704ea-1096-4342-eacc-05e5c5c57ae6"
   },
   "outputs": [],
   "source": [
    "# Visualizing the distribution\n",
    "def after_trans(dataset):\n",
    "  # unwanted feature\n",
    "  drop = ['country', 'state', 'longitude', 'latitude', 'aqi', 'local_time']\n",
    "  dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "  for feature in dataset.columns:\n",
    "    fig = px.histogram(dataset[feature], x=feature)\n",
    "    fig.show()\n",
    "\n",
    "  return\n",
    "\n",
    "after_trans(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA35h318Fg2F"
   },
   "source": [
    "Still we are facing with huge amount of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCfRlduCFLz-",
    "outputId": "dce59bde-2640-4960-d7ae-3a7e742a7037"
   },
   "outputs": [],
   "source": [
    "def check_and_remove_outliers(dataset):\n",
    "    # Unwanted features to drop temporarily\n",
    "    drop = ['country', 'state', 'latitude', 'longitude', 'local_time', 'aqi']\n",
    "    unwanted_data = dataset[drop]  # Store unwanted features\n",
    "    numeric_data = dataset.drop(drop, axis=1)  # Keep only numeric features\n",
    "\n",
    "    # Store all outlier indices\n",
    "    outliers_indices = set()\n",
    "\n",
    "    # Iterate over each numeric feature in the dataset\n",
    "    for feature in numeric_data.columns:\n",
    "        if pd.api.types.is_numeric_dtype(numeric_data[feature]):\n",
    "            Q1 = numeric_data[feature].quantile(0.25)  # First quartile (25th percentile)\n",
    "            Q3 = numeric_data[feature].quantile(0.75)  # Third quartile (75th percentile)\n",
    "            IQR = Q3 - Q1  # Interquartile range\n",
    "\n",
    "            # Calculate bounds for outliers\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Find outlier indices and add them to the set\n",
    "            outliers = numeric_data[(numeric_data[feature] < lower_bound) | (numeric_data[feature] > upper_bound)]\n",
    "            outliers_indices.update(outliers.index.tolist())\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"Feature '{feature}':\")\n",
    "            print(f\" - Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f}\")\n",
    "            print(f\" - Outliers Found: {len(outliers)}\")\n",
    "\n",
    "    # Remove outliers by index from both datasets\n",
    "    numeric_data_cleaned = numeric_data.drop(index=outliers_indices, errors='ignore')\n",
    "    unwanted_data_cleaned = unwanted_data.drop(index=outliers_indices, errors='ignore')\n",
    "\n",
    "    # Reset the index for both datasets\n",
    "    numeric_data_cleaned.reset_index(drop=True, inplace=True)\n",
    "    unwanted_data_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Concatenate the numeric data with the unwanted features\n",
    "    final_dataset = pd.concat([unwanted_data_cleaned, numeric_data_cleaned], axis=1)\n",
    "\n",
    "    return final_dataset, sorted(outliers_indices)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "cleaned_data, removed_outliers = check_and_remove_outliers(transformed_data)\n",
    "\n",
    "# Print the cleaned data and removed outliers\n",
    "print(\"Cleaned Dataset Shape:\", cleaned_data.shape)\n",
    "print(\"Number of Outliers Removed:\", len(removed_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "WEEi3GhvGI8t",
    "outputId": "b957f045-86e6-438d-f8ab-054ec2658bf7"
   },
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sSdGXOxO9cF",
    "outputId": "4f6eee3d-eda5-4209-fe1d-e5c20f0dcf41"
   },
   "outputs": [],
   "source": [
    "def after_outliers(dataset):\n",
    "    # Unwanted features to drop temporarily\n",
    "    drop = ['country', 'state', 'latitude', 'longitude', 'local_time', 'aqi']\n",
    "    unwanted_data = dataset[drop]  # Store unwanted features\n",
    "    numeric_data = dataset.drop(drop, axis=1)  # Keep only numeric features\n",
    "\n",
    "    # Iterate over each numeric feature in the dataset\n",
    "    for feature in numeric_data.columns:\n",
    "        # Calculate the upper and lower limits\n",
    "        Q1 = numeric_data[feature].quantile(0.25)\n",
    "        Q3 = numeric_data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Create arrays of Boolean values indicating the outlier rows\n",
    "        upper_array = np.where(numeric_data[feature] > upper)[0]\n",
    "        lower_array = np.where(numeric_data[feature] < lower)[0]\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Feature: {feature}\")\n",
    "        print(f\" - Upper Bound Outliers Indices: {upper_array}\")\n",
    "        print(f\" - Lower Bound Outliers Indices: {lower_array}\")\n",
    "        print(f\" - Total Outliers: {len(upper_array) + len(lower_array)}\\n\")\n",
    "\n",
    "after_outliers(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dPLON9LzM_1l",
    "outputId": "a788b088-2bc6-426f-aefe-7cf43c9f3c30"
   },
   "outputs": [],
   "source": [
    "# checking graphs after outliers dropped\n",
    "after_trans(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWdgQPTsN37h",
    "outputId": "c035864e-0c7d-4943-c976-f6dcab7edd44"
   },
   "outputs": [],
   "source": [
    "# The total affect of outliers in a dataset\n",
    "print(f\"Total percentage we lost after Outliers: {12048/len(cleaned_data) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb6CWeCWnopt"
   },
   "source": [
    "### Result\n",
    "As you can see above we lost huge amount of data after normal outlier checkup and dropout. In addition, still we have outliers after cleaning outliers and this showes that we can not go with normal outliers checkup and removing.<br>\n",
    "**To handle this:** we need to use Robust Scaler method it won't all outliers but it will manage it to model can handle it. Also keep in mind that those outliers important data and based on countries dangrous air quality outliers happend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hz5lItMFm2-J",
    "outputId": "c6fe39d4-c6d7-4d8a-fba8-f35b7e4b48d9"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def robust_scaler(dataset):\n",
    "  # Drop unwanted features for scaling process\n",
    "  drop = ['country', 'state', 'longitude', 'latitude', 'local_time', 'aqi']\n",
    "  X = dataset.drop(drop, axis=1)\n",
    "  y = dataset['aqi']  # Target variable (AQI)\n",
    "\n",
    "  # Initialize RobustScaler\n",
    "  scaler = RobustScaler()\n",
    "\n",
    "  # Apply RobustScaler to each feature individually\n",
    "  X_scaled = X.apply(lambda column: scaler.fit_transform(column.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "  # Reattach the unwanted features to the scaled dataset\n",
    "  X_scaled['country'] = dataset['country']\n",
    "  X_scaled['state'] = dataset['state']\n",
    "  X_scaled['longitude'] = dataset['longitude']\n",
    "  X_scaled['latitude'] = dataset['latitude']\n",
    "  X_scaled['local_time'] = dataset['local_time']\n",
    "\n",
    "  # Add the target variable 'aqi' back to the dataset\n",
    "  X_scaled['aqi'] = y\n",
    "\n",
    "  # Display the final dataset with both scaled and original columns\n",
    "  print(X_scaled)\n",
    "\n",
    "  return X_scaled\n",
    "\n",
    "robust_data = robust_scaler(transformed_data)\n",
    "robust_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0niDey3Y2IYy",
    "outputId": "46f14b3c-879b-4c46-8b87-efcf711bcc90"
   },
   "outputs": [],
   "source": [
    "# checking the histogram\n",
    "after_trans(robust_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-mzWjWD2XR4",
    "outputId": "283dc0d9-8766-48fa-9341-dd8fb83c4f7f"
   },
   "outputs": [],
   "source": [
    "# lets check total outliers again\n",
    "def total_outliers(dataset):\n",
    "  # Unwanted features to drop temporarily\n",
    "  drop = ['country', 'state', 'latitude', 'longitude', 'local_time', 'aqi']\n",
    "  unwanted_data = dataset[drop]  # Store unwanted features\n",
    "  dataset = dataset.drop(drop, axis=1)  # Keep only numeric features\n",
    "\n",
    "  # Quantile\n",
    "  Q1 = dataset.quantile(0.25)\n",
    "  Q3 = dataset.quantile(0.75)\n",
    "\n",
    "  # Lower and Upper bound\n",
    "  lower = Q1 - 1.5 * (Q3 - Q1)\n",
    "  upper = Q3 + 1.5 * (Q3 - Q1)\n",
    "\n",
    "  # checking for outliers\n",
    "  outlier = ((dataset > upper) | (dataset < lower))\n",
    "\n",
    "  print(outlier.sum())\n",
    "\n",
    "total_outliers(robust_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "3M9iCYXiytO9",
    "outputId": "06588127-6628-482a-df9d-6195ba69a8d8"
   },
   "outputs": [],
   "source": [
    "# checking the statistical analysis of before and after robust scaler\n",
    "# Before Robust Scaler\n",
    "transformed_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "z04GzOoVzCh0",
    "outputId": "d8aae2f3-52df-41f5-a791-9bdd60d15310"
   },
   "outputs": [],
   "source": [
    "# After Robust Scaling\n",
    "robust_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o2ZQQXj3Tca"
   },
   "source": [
    "### Result:\n",
    "After long processing finally we have handled the outliers. But take note that I do not mean solving or making the outliers zero but we have transfored from huge differences to less in values but yes still there is problem in distribution and if we remove those ourtliers it would damage important data such countries like India, Pakistan, Iran, Afghanistan and some other countries records will be deleted. To avoid this we have used Robust Scaler and now with the sitution above described in statistical analysis I wishes it wount hurt to much the models and we will use some benificial activition fuction to solve it.\n",
    "So with all these condition we wishes still be positive and bear to go for further analysis. 😊😊😊😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😎😃😃😃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "edb99dca49204cecbdb94497a8d6e61e",
      "94e277e7d3a8478ebcfc02a9f6919b27",
      "6a3046ef6b354a60a454f66493687238",
      "5763d2305bce477b882cd8de01ac7234",
      "ecdc5600be8341d6beee2e83aa32bbdc",
      "7f6de81b902e4869815d2f640833dbc4",
      "33126f008ac848488c2b73d6ee785266",
      "26451240ab1a4c15945ac45357189297",
      "abf3e013de29493ab8ae2b223a4a407e",
      "a3c8b36452214a0780867253b04fd0bb",
      "41fc55a3f10f4df6b04a57d9bba7862b",
      "67a90a565f0f408eb89ad7814c2dbd2c",
      "7745bf5f386f4b13ba7642f477669054",
      "bc4e2d1534d94c60bfe6665eac43ccbb",
      "eddb1bb875fb4249bc3ee16551892a37",
      "82cbbd9a83a149ae9e3edecc25826e6d",
      "001061702153468f9fd7770255f864ff",
      "8a17b211bada4143b3109994509ababc",
      "bf4a71b2cd134da9b472dbe715e9967f",
      "2c5489b54f5f40b38d579de2d3144d4d",
      "26c02ba114b048448eddcef249a2f57c",
      "9fc8c4c8d2294b85a3ede1c812e5252c",
      "dc386402672843bb8e82ba03056c1586",
      "376856a786934340bc73306a4be2dd0f",
      "7663f42f071d4ab18fbb89c4e8d9c117",
      "b5edf73e0fd14b8e8cb34202f910143d",
      "ffb3d3bef5ca453eba6b5b0dab9daa2b",
      "dbb2fde6856747a6872e3d9e1fd31cd8",
      "8c452c291c254ce7a940e659e2d6b742",
      "e270ca8a8f2945109e57f53bc2eb8e04",
      "8ea860ab381f46269a53a2291b71f63c",
      "29281ed12ab6439baf461333bd1d9ad2",
      "62493a82c55d48738a95356045a60705"
     ]
    },
    "id": "QnyZRfOd4CQU",
    "outputId": "ae74b7c5-2f17-404f-ccb2-b516f7f76317"
   },
   "outputs": [],
   "source": [
    "# checking full data report using profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(clean_data, title='Dataset Report')\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptEmtsgCUR7D"
   },
   "source": [
    "### Next Step:\n",
    "* Kabul local data has missmatch date format and required to standardize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "45YF84tUI3Hy",
    "outputId": "989ae062-61cf-4522-a3cb-572e6203afcb"
   },
   "outputs": [],
   "source": [
    "# Time to handle Afghanistan local time\n",
    "def kabul_date_format(dataset):\n",
    "    # Select rows where 'state' is 'Kabul'\n",
    "    mask = dataset['state'] == 'Kabul'\n",
    "\n",
    "    # Identify rows needing conversion (non-standard date format)\n",
    "    needs_conversion = ~dataset.loc[mask, 'local_time'].str.match(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$', na=False)\n",
    "\n",
    "    # Convert non-standard dates to datetime format\n",
    "    dataset.loc[mask & needs_conversion, 'local_time'] = pd.to_datetime(\n",
    "        dataset.loc[mask & needs_conversion, 'local_time'],\n",
    "        format='%d/%m/%Y',\n",
    "        errors='coerce',\n",
    "        dayfirst=True\n",
    "    )\n",
    "\n",
    "    # Add missing time details for converted dates\n",
    "    dataset.loc[mask & needs_conversion, 'local_time'] = dataset.loc[mask & needs_conversion, 'local_time'].apply(\n",
    "        lambda x: x.replace(hour=0, minute=0, second=0) if not pd.isnull(x) else x\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "time_data = kabul_date_format(robust_data)\n",
    "\n",
    "time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "-3pZ2PYg1WB1",
    "outputId": "02f50c7d-d92a-4696-dbfd-8f8a737af801"
   },
   "outputs": [],
   "source": [
    "# Checking wheather there is missing value occures during time convertion format\n",
    "time_data[time_data['local_time'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9vu0g5wg7RRz",
    "outputId": "b965ae05-d996-4783-a09e-ec62ed2f35a3"
   },
   "outputs": [],
   "source": [
    "# lets compare with exact row time format and then fill it\n",
    "miss_time = data['local_time'].loc[54881]\n",
    "time_data['local_time'] = time_data['local_time'].fillna(pd.Timestamp(miss_time))\n",
    "time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "vDyFEkPi1LLo",
    "outputId": "c947a27e-049a-488d-94b5-0636256f3609"
   },
   "outputs": [],
   "source": [
    "# lets check kabul local time\n",
    "time_data[time_data['state'] == 'Kabul']['local_time'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcWkUaEhKED8"
   },
   "source": [
    "### Result:\n",
    "So far we have done these missions:\n",
    "* Converting kabul time format to standard (ISO8601) format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxl_0TUfcwjS"
   },
   "source": [
    "### **Feature Engineering:**\n",
    "`Info:`\n",
    "Feature engineering is a very important step in machine learning. Feature engineering refers to the process of designing artificial features into an algorithm. These artificial features are then used by that algorithm in order to improve its performance, or in other words, reap better results.<br>\n",
    "**`Our Policy:`**<br>\n",
    "In this section we are going to check these steps:\n",
    "* Correlation and Covariance of features.\n",
    "* PCA (Principal Component Analysis)\n",
    "* Feature Selection\n",
    "* Pairwise Interaction Terms\n",
    "* Handle mostly zero value in a feature\n",
    "* Rolling and Laging to create trend and seasonality.\n",
    "* Balancing data based on countries using ARIMA\n",
    "method\n",
    "* clustring the data based on longitude and latitude\n",
    "* Balancing the AQI using smoth method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "GBHZaeO9eSnJ",
    "outputId": "a56fb565-2ffc-4de3-be8a-ef2a9b789296"
   },
   "outputs": [],
   "source": [
    "# lets go with the first step of Feature Engineering (Correlation and Covariance) :)\n",
    "# This correlation and covarience do not support (NO, NH3) feature but the next one will\n",
    "def corr_and_cov(dataset, drop):\n",
    "    dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "    # Compute correlation and covariance matrices\n",
    "    corr = dataset.corr()\n",
    "    cov = dataset.cov()\n",
    "\n",
    "    # Create the plots\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Correlation heatmap\n",
    "    plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', cbar=True, square=True)\n",
    "    plt.title('Data Correlation')\n",
    "\n",
    "    # Covariance heatmap\n",
    "    plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
    "    sns.heatmap(cov, annot=True, cmap='viridis', cbar=True, square=True)\n",
    "    plt.title('Data Covariance')\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Drop unwanted features\n",
    "drop = ['latitude', 'longitude', 'local_time', 'state', 'country', 'no', 'nh3']\n",
    "corr_and_cov(time_data, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "KAmAw2bexSo4",
    "outputId": "1ceaf3d6-559a-4a1d-b587-aac18512a92c"
   },
   "outputs": [],
   "source": [
    "# correlation and covarience with no and nh3\n",
    "drop = ['latitude', 'longitude', 'local_time', 'state', 'country']\n",
    "corr_and_cov(time_data, drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMAg6Z6jkile"
   },
   "source": [
    "### Result:\n",
    "Based on the description of features. (pm2.5 and pm10) have a strong correlation that indicate some common values. And also NH3 and NO has less effeciency on AQI. We need to check again using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "COD24xI-hDHx",
    "outputId": "bdf28bd2-7de1-4feb-8f58-0ba7a276e8f3"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def perform_pca(dataset, n_components=None):\n",
    "    # Unwanted features to drop temporarily\n",
    "    drop = ['country', 'state', 'latitude', 'longitude', 'local_time', 'aqi']\n",
    "    unwanted_data = dataset[drop]  # Store unwanted features\n",
    "    numeric_data = dataset.drop(drop, axis=1)  # Numeric features for PCA\n",
    "\n",
    "    # Standardize the numeric data for PCA\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(numeric_data)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(standardized_data)\n",
    "\n",
    "    # Create a DataFrame for the principal components\n",
    "    pca_columns = [f\"PC{i+1}\" for i in range(principal_components.shape[1])]\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=pca_columns)\n",
    "\n",
    "    # Combine original dataset (unscaled numeric data) with PCA components and unwanted data\n",
    "    final_dataset = pd.concat([dataset.reset_index(drop=True), pca_df], axis=1)\n",
    "\n",
    "    # Variance explained by each component\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    # Plot 1: Explained variance ratio\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, align='center', label='Explained Variance')\n",
    "    plt.step(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), where='mid', color='red', label='Cumulative Variance')\n",
    "    plt.xlabel('Principal Components')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Feature contributions to each principal component\n",
    "    feature_names = numeric_data.columns  # Original feature names\n",
    "    pca_components = pca.components_  # PCA component weights\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['blue', 'green', 'orange', 'red', 'purple']  # Different colors for each PC\n",
    "\n",
    "    for i, pc in enumerate(pca_columns):\n",
    "        plt.bar(feature_names, pca_components[i], color=colors[i % len(colors)], alpha=0.7, label=pc)\n",
    "\n",
    "    plt.title('Feature Contributions to Each Principal Component', fontsize=16)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('Contribution', fontsize=12)\n",
    "    plt.xticks(rotation=45, fontsize=10)\n",
    "    plt.legend(title='Principal Components', fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return final_dataset, explained_variance_ratio\n",
    "\n",
    "pca_result, explained_variance = perform_pca(time_data, n_components=5)\n",
    "\n",
    "\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(explained_variance)\n",
    "print(sum(explained_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hrCkl-crFko"
   },
   "source": [
    "### Result:\n",
    "As we can see the varience ratio indicate how each pc's contribution to the dataset and the first pc has the most highest values of data inside. In addition totally the five pcs can include `91%` of the data and we lost `9%` of the data, so we can increase the principle component or just continue with the feature Engineering. Next, second graph showes how each feature contribute to the pcs.<br>\n",
    "We will go further steps of `Feature Engineering` and after that make decision about PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjBLN2ry3uzs"
   },
   "source": [
    "### Next steps:\n",
    "* Adding features based on Years, Months, Days, Hour and Minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "-fGxUGyg3MPt",
    "outputId": "fb7ea2d2-ec93-489b-bbdb-0ad5ce7b3390"
   },
   "outputs": [],
   "source": [
    "# Now lets handle local_time\n",
    "# Add features for year, month, day, hour, and minute\n",
    "def add_time_features(dataset):\n",
    "    dataset['local_time'] = pd.to_datetime(dataset['local_time'], errors='coerce')\n",
    "\n",
    "    # Extract and add features\n",
    "    dataset['year'] = dataset['local_time'].dt.year\n",
    "    dataset['month'] = dataset['local_time'].dt.month\n",
    "    dataset['day'] = dataset['local_time'].dt.day\n",
    "    dataset['hour'] = dataset['local_time'].dt.hour\n",
    "    dataset['dayofweek'] = dataset['local_time'].dt.dayofweek\n",
    "\n",
    "    return dataset\n",
    "\n",
    "time_data = add_time_features(time_data)\n",
    "\n",
    "time_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "iBDPZDIf7CYj",
    "outputId": "a1f0a8e5-7105-40f9-80da-93a286b3ba8d"
   },
   "outputs": [],
   "source": [
    "# Time to drop local_time feature\n",
    "time_data.drop('local_time', axis=1, inplace=True)\n",
    "time_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elAJzCNWSvzl"
   },
   "source": [
    "### Time to handle imbalance data and balance it based on countries.\n",
    "why selecting countries?<br>\n",
    "Well mostly we do not have sensors in every state or province of countries to we select based on their states. This is why we have select countries and in some countries just there is one or sensors like Afghanistan. So, that is why we have selected based on countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDft8ybmx4gU",
    "outputId": "d39bbde1-07e4-4264-f082-abb5c4c8c34f"
   },
   "outputs": [],
   "source": [
    "# lets check again the countries records in dataset\n",
    "np.array(time_data['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VacaxXvOx4c9",
    "outputId": "14fa93a5-4d0e-4ef1-fe12-100c0ca3d4c4"
   },
   "outputs": [],
   "source": [
    "# its percentage\n",
    "np.array(time_data['country'].value_counts() / len(time_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZUJjpWjxOFs"
   },
   "outputs": [],
   "source": [
    "# Save time data\n",
    "# time_data.to_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/time_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcLZugHJx4aQ",
    "outputId": "86611241-9e0a-4605-cf65-755f73eeef6d"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Function to generate synthetic data for a given country's data\n",
    "def generate_synthetic_data(country_data, target_count=350):\n",
    "    \"\"\"\n",
    "    Generate synthetic time-series data for a given country's data using ARIMA.\n",
    "    \"\"\"\n",
    "    current_count = len(country_data)\n",
    "\n",
    "    if current_count >= target_count:\n",
    "        return country_data  # No need to generate additional data\n",
    "\n",
    "    # Calculate the number of records to generate\n",
    "    additional_count = target_count - current_count\n",
    "\n",
    "    # Generate synthetic data for each feature\n",
    "    synthetic_records = []\n",
    "    for feature in country_data.columns.drop(['country', 'state', 'longitude', 'latitude', 'year', 'month', 'day', 'hour', 'dayofweek']):\n",
    "        # Fit ARIMA on the existing data\n",
    "        model = ARIMA(country_data[feature], order=(1, 1, 1))  # Adjust order if necessary\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Generate synthetic values\n",
    "        forecast = model_fit.forecast(steps=additional_count)\n",
    "\n",
    "        # Ensure proper types for specific features\n",
    "        if feature == 'aqi':\n",
    "            forecast = np.round(forecast).astype(int)  # Ensure aqi remains integer\n",
    "        synthetic_records.append(forecast)\n",
    "\n",
    "    # Combine synthetic data into a DataFrame\n",
    "    synthetic_data_df = pd.DataFrame(synthetic_records).T\n",
    "    synthetic_data_df.columns = country_data.columns.drop(['country', 'state', 'longitude', 'latitude', 'year', 'month', 'day', 'hour', 'dayofweek'])\n",
    "\n",
    "    # Assign fixed values for `country`, `state`, `longitude`, `latitude`\n",
    "    synthetic_data_df['country'] = country_data['country'].iloc[0]\n",
    "    synthetic_data_df['state'] = country_data['state'].iloc[0]\n",
    "    synthetic_data_df['longitude'] = country_data['longitude'].iloc[0]\n",
    "    synthetic_data_df['latitude'] = country_data['latitude'].iloc[0]\n",
    "\n",
    "    # Assign timestamp to synthetic data\n",
    "    synthetic_data_df['year'] = country_data['year'].iloc[-1]\n",
    "    synthetic_data_df['month'] = country_data['month'].iloc[-1]\n",
    "    synthetic_data_df['day'] = country_data['day'].iloc[-1] + np.arange(1, additional_count + 1) // 24\n",
    "    synthetic_data_df['hour'] = (country_data['hour'].iloc[-1] + np.arange(1, additional_count + 1)) % 24\n",
    "    synthetic_data_df['dayofweek'] = country_data['dayofweek'].iloc[-1]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    balanced_data = pd.concat([country_data.reset_index(drop=True), synthetic_data_df])\n",
    "\n",
    "    return balanced_data\n",
    "\n",
    "# Main balancing function\n",
    "def balance_dataset_with_arima(dataset, target_count=320):\n",
    "    \"\"\"\n",
    "    Balance the dataset by generating synthetic records for underrepresented countries.\n",
    "    \"\"\"\n",
    "    # Group by country and count records\n",
    "    country_counts = dataset['country'].value_counts()\n",
    "\n",
    "    # Identify underrepresented countries\n",
    "    underrepresented_countries = country_counts[country_counts < target_count].index\n",
    "\n",
    "    # Balance the dataset\n",
    "    balanced_dataset = []\n",
    "    for country in underrepresented_countries:\n",
    "        country_data = dataset[dataset['country'] == country]\n",
    "        balanced_country_data = generate_synthetic_data(country_data, target_count=target_count)\n",
    "        balanced_dataset.append(balanced_country_data)\n",
    "\n",
    "    # Include countries already having enough records\n",
    "    sufficient_data_countries = dataset[~dataset['country'].isin(underrepresented_countries)]\n",
    "    balanced_dataset.append(sufficient_data_countries)\n",
    "\n",
    "    # Combine all balanced data\n",
    "    balanced_dataset = pd.concat(balanced_dataset)\n",
    "    balanced_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return balanced_dataset\n",
    "\n",
    "# Balance the dataset\n",
    "balanced_dataset = balance_dataset_with_arima(time_data, target_count=320)\n",
    "\n",
    "# Validate record counts\n",
    "balanced_country_counts = balanced_dataset['country'].value_counts()\n",
    "print(balanced_country_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mH-9z-6Ix4X7",
    "outputId": "9083f4f3-bab0-4aca-d0ac-dc87014da9f4"
   },
   "outputs": [],
   "source": [
    "# Checking tha balance of dataset of after implementing ARIMA\n",
    "np.array(balanced_dataset['country'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfP8_c8dNzQ8",
    "outputId": "3a6036bd-1f99-4b7f-fd9b-cef4eb00b260"
   },
   "outputs": [],
   "source": [
    "# Checking the percentage of each country:\n",
    "np.array(balanced_dataset['country'].value_counts() / len(balanced_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ergfQUFuNzOt",
    "outputId": "0e853b95-195c-4d9c-c588-d4f39c271262"
   },
   "outputs": [],
   "source": [
    "# Checking the duplicated values agian\n",
    "balanced_dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3tXEW4CfDpT"
   },
   "source": [
    "Dropping country and state features of countries and save the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPzfryB43_Fe"
   },
   "outputs": [],
   "source": [
    "# Right now we do not need country and state features. So lets drop them.\n",
    "balanced_dataset.drop(['country', 'state'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTEKE2CFOPFa"
   },
   "outputs": [],
   "source": [
    "# Saving this new dataset\n",
    "# balanced_dataset.to_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/balanced_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7N-ZYPEx4Vb"
   },
   "outputs": [],
   "source": [
    "# Now lets import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "VAHH7Yh0sRpA",
    "outputId": "4255f345-c95d-486e-c8d9-d7407a8cab0b"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "balanced_dataset = pd.read_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/balanced_dataset.csv\")\n",
    "balanced_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zib0WtpbfO5Y"
   },
   "source": [
    "Checking Correlation and Covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "DO_D4M6l4WQn",
    "outputId": "9429871e-4232-498a-c5a7-9a385be33043"
   },
   "outputs": [],
   "source": [
    "# lets check correlation and covarince.\n",
    "def corr_and_cov(dataset, drop):\n",
    "    dataset = dataset.drop(drop, axis=1)\n",
    "    # Compute correlation and covariance matrices\n",
    "    corr = dataset.corr()\n",
    "    cov = dataset.cov()\n",
    "\n",
    "    # Create the plots\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Correlation heatmap\n",
    "    plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', cbar=True, square=True)\n",
    "    plt.title('Data Correlation')\n",
    "\n",
    "    # Covariance heatmap\n",
    "    plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
    "    sns.heatmap(cov, annot=True, cmap='viridis', cbar=True, square=True)\n",
    "    plt.title('Data Covariance')\n",
    "\n",
    "    # Adjust layout and show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "drop = ['longitude', 'latitude', 'year', 'month', 'day', 'hour', 'dayofweek']\n",
    "corr_and_cov(balanced_dataset, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "OGVw3Ne14WNN",
    "outputId": "7e213112-9998-4941-f310-c386d8873ea6"
   },
   "outputs": [],
   "source": [
    "# Lets check without nh3 and no\n",
    "drop = ['longitude', 'latitude', 'year', 'month', 'day', 'hour', 'dayofweek', 'nh3', 'no']\n",
    "corr_and_cov(balanced_dataset, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "f358Tg7_4WKW",
    "outputId": "fe2ba8cc-bb89-418d-f902-21b18e4635bb"
   },
   "outputs": [],
   "source": [
    "# Lets check without nh3, p,2.5 and no\n",
    "drop = ['longitude', 'latitude', 'year', 'month', 'day', 'hour', 'dayofweek', 'nh3', 'no', 'pm10']\n",
    "corr_and_cov(balanced_dataset, drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_vNQiyrfb4q"
   },
   "source": [
    "**Feature Importance Using RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "YHGzWztOt6zO",
    "outputId": "46528dd8-fc91-4a0b-81f4-927d685b452f"
   },
   "outputs": [],
   "source": [
    "# Implementing feature importance without nh3, no\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def feature_importance_with_rf(dataset, drop):\n",
    "  X = dataset.drop(drop, axis=1)\n",
    "  y = dataset['aqi']\n",
    "\n",
    "  # Split data into training and testing sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "  # Initialize a Random Forest Classifier (you can also use RandomForestRegressor for regression tasks)\n",
    "  rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "  # Train the Random Forest model\n",
    "  rf.fit(X_train, y_train)\n",
    "\n",
    "  # Get feature importance scores\n",
    "  importances = rf.feature_importances_\n",
    "\n",
    "  # Create a DataFrame to display feature importance\n",
    "  feature_importance_df = pd.DataFrame({\n",
    "      'Features': X.columns,\n",
    "      'Importance': importances\n",
    "  })\n",
    "\n",
    "  # Sort by importance\n",
    "  feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "  # Plot feature importance\n",
    "  plt.figure(figsize=(10,6))\n",
    "  plt.barh(feature_importance_df['Features'], feature_importance_df['Importance'])\n",
    "  plt.xlabel('Importance')\n",
    "  plt.title('Feature Importance')\n",
    "  plt.show()\n",
    "\n",
    "drop = ['latitude', 'longitude', 'nh3', 'no', 'aqi']\n",
    "feature_importance_with_rf(time_data, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "WwIO5csNvgey",
    "outputId": "345a9ca6-9132-4674-89b0-aa58e32a8f72"
   },
   "outputs": [],
   "source": [
    "# Feature importance with no and nh3\n",
    "drop = ['latitude', 'longitude', 'aqi']\n",
    "feature_importance_with_rf(time_data, drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr_jVC4XT6JN"
   },
   "source": [
    "### Feature Importance with MI\n",
    "In this step we are going to check Classification with and without (NH3, NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "id": "N9CWi_aCxpky",
    "outputId": "7b5cb370-69e5-4267-f971-4ceabe7a001c"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "def calculate_mutual_information(dataset, target, task_type, drop_columns, n_bins=None, labels=None):\n",
    "    \"\"\"\n",
    "    Calculate and visualize Mutual Information for dataset features with robust binning.\n",
    "    \"\"\"\n",
    "    # Ensure dataset and target have the same number of rows\n",
    "    if len(dataset) != len(target):\n",
    "        raise ValueError(\"Dataset and target must have the same number of rows.\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    dataset = dataset.drop(drop_columns, axis=1)\n",
    "\n",
    "    # Dynamically adjust bins based on target distribution if classification task\n",
    "    if task_type == 'classification' and n_bins is not None:\n",
    "        target_min, target_max = target.min(), target.max()\n",
    "        bins = np.linspace(target_min, target_max, n_bins + 1)  # Dynamic bin edges\n",
    "        print(f\"Dynamic bins: {bins}\")  # Debug bin edges\n",
    "        if labels is None:\n",
    "            labels = range(1, n_bins + 1)  # Default labels as integers\n",
    "        target = pd.cut(target, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "        # Check and handle NaN values in target after binning\n",
    "        if target.isna().sum() > 0:\n",
    "            print(\"Warning: NaN values found in target after binning. Dropping invalid rows.\")\n",
    "            valid_indices = target.notna()\n",
    "            dataset = dataset[valid_indices]\n",
    "            target = target[valid_indices]\n",
    "\n",
    "    # Choose mutual information function based on task type\n",
    "    if task_type == 'classification':\n",
    "        mi = mutual_info_classif(dataset, target, random_state=42)\n",
    "    elif task_type == 'regression':\n",
    "        mi = mutual_info_regression(dataset, target, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task_type. Use 'classification' or 'regression'.\")\n",
    "\n",
    "    # Create a Pandas Series of MI scores\n",
    "    mi_scores = pd.Series(mi, index=dataset.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    # Plot Mutual Information Scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mi_scores.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Mutual Information Scores', fontsize=14)\n",
    "    plt.ylabel('Mutual Information', fontsize=12)\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "\n",
    "drop_columns = ['latitude', 'longitude', 'no', 'nh3']\n",
    "\n",
    "# For classification with dynamic bins\n",
    "n_bins = 5\n",
    "mi_scores = calculate_mutual_information(\n",
    "    dataset=balanced_dataset.drop('aqi', axis=1),\n",
    "    target=balanced_dataset['aqi'],\n",
    "    task_type='classification',\n",
    "    drop_columns=drop_columns,\n",
    "    n_bins=n_bins  # Automatically adjusts bins\n",
    ")\n",
    "\n",
    "print(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "HA_-jFeuxphT",
    "outputId": "df64d694-c706-433f-f596-5307be58a658"
   },
   "outputs": [],
   "source": [
    "# MI with NO and NH3\n",
    "drop_columns = ['latitude', 'longitude']\n",
    "\n",
    "# For classification with dynamic bins\n",
    "n_bins = 5\n",
    "mi_scores = calculate_mutual_information(\n",
    "    dataset=balanced_dataset.drop('aqi', axis=1),\n",
    "    target=balanced_dataset['aqi'],\n",
    "    task_type='classification',\n",
    "    drop_columns=drop_columns,\n",
    "    n_bins=n_bins  # Automatically adjusts bins\n",
    ")\n",
    "\n",
    "print(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beHSW3CpU0Dx"
   },
   "source": [
    "### Result:\n",
    "As we have seen Correlation and Covarience of NH3 and NO. They had poor relationship with AQI and the main factor was that they both have the same relation with CO and it showes some imbalance and by the way NO2 and CO and PM2.5 have can support NH3 and NO. In addition most sensors and countries do not count these two factors as AIR quality metric. But also they have their danger if their values are huge in air. So based on these we are going to drop these features, and lets see what will happen in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "TrpjMN73xpel",
    "outputId": "898b92b0-d45d-46e4-b268-e56b67546fc3"
   },
   "outputs": [],
   "source": [
    "# Droping NO and NH3\n",
    "droped_data = balanced_dataset.drop(['nh3', 'no'], axis=1)\n",
    "droped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L78-uqCzVHI7"
   },
   "source": [
    "### Zero values:\n",
    "Lets check zero values with ploting using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aivUdcrQx4Qm",
    "outputId": "e6f6e9f1-835c-4711-b498-22832676bd39"
   },
   "outputs": [],
   "source": [
    "# Visualize the zero values of each feature using a scatter plot\n",
    "def zero_value(dataset):\n",
    "    # Unwanted features to drop\n",
    "    drop = ['longitude', 'latitude', 'aqi', 'year', 'month', 'day', 'hour', 'dayofweek']\n",
    "    dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "    # Iterate through each feature\n",
    "    for feature in dataset.columns:\n",
    "        # Get value counts and convert to a DataFrame\n",
    "        value_counts = dataset[feature].value_counts().reset_index()\n",
    "        value_counts.columns = [f\"{feature}_value\", \"count\"]\n",
    "\n",
    "        # Plot using Plotly\n",
    "        fig = px.scatter(\n",
    "            value_counts,\n",
    "            x=f\"{feature}_value\",\n",
    "            y=\"count\",\n",
    "            title=f\"Different values of {feature}\",\n",
    "            labels={f\"{feature}_value\": \"Value\", \"count\": \"Count\"}\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "zero_value(droped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybSiNU1lvKaW"
   },
   "source": [
    "### K-means Clustring:\n",
    "We have different locations cordinates and we can not goes for all of them individually. So what we will do is using `K-mean cluster` algorithm to handle the cordinate and then use its ID.\n",
    "* Finding Optimal cluster value.\n",
    "  * Using elbow method\n",
    "  * elbow and silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "HgMoCdErx4OO",
    "outputId": "d5255460-95a7-4f9b-c529-68deb142bc1e"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def optimal_cluster(dataset):\n",
    "\n",
    "  # Normalize latitude and longitude\n",
    "  scaler = StandardScaler()\n",
    "  dataset[['latitude', 'longitude']] = scaler.fit_transform(dataset[['latitude', 'longitude']])\n",
    "\n",
    "  # Test k values\n",
    "  distortions = []\n",
    "  K = range(10, 101, 10)  # Test for 10 to 100 clusters\n",
    "  for k in K:\n",
    "      kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "      kmeans.fit(dataset[['latitude', 'longitude']])\n",
    "      distortions.append(kmeans.inertia_)\n",
    "\n",
    "  # Plot the Elbow Curve\n",
    "  plt.plot(K, distortions, marker='o')\n",
    "  plt.xlabel('Number of Clusters (k)')\n",
    "  plt.ylabel('Distortion')\n",
    "  plt.title('Elbow Method for Optimal Clusters')\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "optimal_cluster(droped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q--HuhEvx4Lf",
    "outputId": "bdf9e0b7-7f79-43d5-f90a-774726bc7b43"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def optimal_cluster(dataset):\n",
    "    # Normalize latitude and longitude\n",
    "    scaler = StandardScaler()\n",
    "    dataset[['latitude', 'longitude']] = scaler.fit_transform(dataset[['latitude', 'longitude']])\n",
    "\n",
    "    # Elbow Method\n",
    "    distortions = []\n",
    "    K = range(10, 101, 10)  # Test for 10 to 100 clusters\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(dataset[['latitude', 'longitude']])\n",
    "        distortions.append(kmeans.inertia_)\n",
    "\n",
    "    # Plot the Elbow Curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K, distortions, marker='o', linestyle='--', color='b')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('Elbow Method for Optimal Clusters')\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # Silhouette Method\n",
    "    silhouette_scores = []\n",
    "    K_silhouette = range(10, 51, 5)  # Test for 10 to 50 clusters (for faster computation)\n",
    "    for k in K_silhouette:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(dataset[['latitude', 'longitude']])\n",
    "        silhouette_scores.append(silhouette_score(dataset[['latitude', 'longitude']], labels))\n",
    "\n",
    "    # Plot the Silhouette Scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K_silhouette, silhouette_scores, marker='o', linestyle='--', color='g')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Method for Optimal Clusters')\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # Find optimal k from Silhouette Scores\n",
    "    optimal_k = K_silhouette[silhouette_scores.index(max(silhouette_scores))]\n",
    "    print(f\"Optimal number of clusters based on Silhouette Method: {optimal_k}\")\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "\n",
    "optimal_k = optimal_cluster(droped_data)\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCEfiIC78cLp"
   },
   "source": [
    "### Result:\n",
    "Based on the output of Elbow and Silhouette method and our desire of our project such that users location cordinate is important for out project to predict and classify well based on their location. and that is why we will go with `K = 50` or `Optimal cluster = 50`.\n",
    "### Next Step:\n",
    "Now it is time to we create our reagion_id feature based on k_means clustring using k value of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "0vXuKMzoPCly",
    "outputId": "1e310965-4b65-4fef-d63d-bac90a063748"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# Step 1: Shuffle the dataset\n",
    "def shuffle_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Shuffle the dataset to ensure data is mixed before clustering.\n",
    "    \"\"\"\n",
    "    return dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 2: Define the function for region clustering\n",
    "def add_region_id(dataset, k=50):\n",
    "    \"\"\"\n",
    "    Add region_id feature to the dataset based on latitude and longitude clustering.\n",
    "    \"\"\"\n",
    "    # Shuffle the dataset\n",
    "    dataset = shuffle_dataset(dataset)\n",
    "\n",
    "    # Normalize latitude and longitude\n",
    "    scaler = StandardScaler()\n",
    "    dataset[['latitude', 'longitude']] = scaler.fit_transform(dataset[['latitude', 'longitude']])\n",
    "    with open(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/cluster-scaler.sav\", 'wb') as f:\n",
    "        joblib.dump(scaler, f)\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    dataset['region_id'] = kmeans.fit_predict(dataset[['latitude', 'longitude']])\n",
    "\n",
    "    # Save the KMeans model for future use\n",
    "    with open('/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/kmeans_model.pkl', 'wb') as file:\n",
    "      pickle.dump(kmeans, file)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Step 3: Apply clustering to the dataset\n",
    "cluster_data = add_region_id(droped_data)\n",
    "cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ehXZABajVhM9",
    "outputId": "f167dde1-30a2-4427-f2a8-ef3b7c2e7dd4"
   },
   "outputs": [],
   "source": [
    "# Lets check count value of region_id\n",
    "cluster_data['region_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "rJ8ORfFkVxla",
    "outputId": "7e7db6b3-a1fd-41c6-d497-2e88f7e195c2"
   },
   "outputs": [],
   "source": [
    "# Now lets drop latitude and longitude features\n",
    "cluster_data = cluster_data.drop(['latitude', 'longitude'], axis=1)\n",
    "cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km10zdjKWsSS"
   },
   "source": [
    "### Result:\n",
    "Perfect we have completed long journey. And we congertulate you.🎉🎉🎉🎉🎉<br>\n",
    "Still you have to bare is for a small `EDA` Check up. to make sure everything smooths well.<br>\n",
    "`Enjoy`😊😎😊😎😊😎\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "oHbJP7BUWjJN",
    "outputId": "6fd27015-885a-4c26-8aab-981ce4cc92b6"
   },
   "outputs": [],
   "source": [
    "# Simple EDA check up, to make sure of everything\n",
    "# Null values\n",
    "cluster_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17fZmYJjX7VG",
    "outputId": "a15d5fb6-5418-47b9-b9dc-610c0075a4ca"
   },
   "outputs": [],
   "source": [
    "# Infinity value check up\n",
    "np.array(np.isinf(cluster_data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEI3KTngYGCe",
    "outputId": "4ccedead-b9c8-4c08-8e1f-bbae12a51edf"
   },
   "outputs": [],
   "source": [
    "# Checking Duplicate\n",
    "cluster_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "7MJScTO2YWXJ",
    "outputId": "f196c44d-cc40-4cdc-a54a-1cfac5eb9b81"
   },
   "outputs": [],
   "source": [
    "# Checking is dataframe\n",
    "cluster_data[cluster_data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pf8tQauLc1CK"
   },
   "outputs": [],
   "source": [
    "# Lets drop duplicated feature\n",
    "cluster_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vpHT-FEdTrm",
    "outputId": "3cf112be-cf16-49b3-8ff3-42ee7fd61aa8"
   },
   "outputs": [],
   "source": [
    "# lets check again\n",
    "cluster_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "JlYQbAlvdYsc",
    "outputId": "d3be4327-f681-4751-fa05-a8d1d3c76dc9"
   },
   "outputs": [],
   "source": [
    "# Function to visualize all data except 'aqi'\n",
    "def all_data_vis(dataset):\n",
    "    # Drop the 'aqi' column\n",
    "    drop = ['aqi', 'year', 'month', 'day', 'hour', 'dayofweek', 'region_id']\n",
    "    dataset = dataset.drop(drop, axis=1)\n",
    "\n",
    "    # Melt the dataset to have all features in one column for easy plotting\n",
    "    dataset_melted = dataset.melt(var_name='Feature', value_name='Value')\n",
    "\n",
    "    # Create the scatter plot with Plotly Express\n",
    "    fig = px.scatter(\n",
    "        dataset_melted,\n",
    "        x=dataset_melted.index,\n",
    "        y='Value',\n",
    "        color='Feature',\n",
    "        title=\"Scatter Plot of Main features\",\n",
    "        labels={'Value': 'Feature Value', 'index': 'Index'},\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_font_size=20,\n",
    "        legend_title_text='Feature',\n",
    "        xaxis=dict(title='Index'),\n",
    "        yaxis=dict(title='Value'),\n",
    "        legend=dict(font=dict(size=10)),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "all_data_vis(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "m9IArZ3meosJ",
    "outputId": "60fae5c2-d960-418c-e2b8-7dfb5e2439b3"
   },
   "outputs": [],
   "source": [
    "# Statistical analysis of data\n",
    "cluster_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "5aa10c7032454e35b137872c12653e6a",
      "5c21c3a4e1984135ab878b6f4ec1c473",
      "02abdd5c0ce0425c81f3163d3331efd7",
      "37105ee2c88b4c5a825cc3022171755c",
      "e479a688365b46efba5896c245058f75",
      "13070a758e0e4e2bb3c849b579eb433a",
      "390c2edfdd0c49f897802e31372f22fa",
      "f855a39c8fc445ecb9a67e3aa4982ae3",
      "7ac585964aa34317bb33c4b91deb2dce",
      "47dad0c29bf0462096bd9e78de498e7c",
      "adf49bc535494cb98a1e55789be96321",
      "ae5e2a10b7844395a0788362043faa3c",
      "827f412fd92c404d9882e86cc85dd337",
      "8138042bed69456bb5e007b29c35c68b",
      "cb216a9109934d43be73e03e84667867",
      "b18b23098fe840f5942746c8f0e259e6",
      "7bd1dba9b16144bda916ff5d0fe14ab3",
      "235120c6c15a4b98962b25b758a4435c",
      "0723a319701a4379a86a5ee174cc125d",
      "99f48c9e7fdc4e48902d9fb3f037d7c2",
      "b3e7aacb44ce4f37b91fad0217fc7757",
      "39c827bad00e403585e7dc9970ea2338",
      "44eb0f14d41d4607b7627e359d26e59f",
      "b2d4feeed63f4404aaa3466d1b21cc1b",
      "b3d37dd129aa400bb8272445e2677d3f",
      "dc1825f4fef246fb9df99d530ba1d89f",
      "19ba0f039a654cd9b15eeeeab582956d",
      "14418179454043d8a01569bd17efb2a6",
      "947ad21244f74946847a8f0af1a73dd7",
      "58e34bb37d7747358979a887b937930c",
      "ff2e2ee7304b49818de1d6cf4ebede7c",
      "3e6af4fbe22f4dc8b8750a044a2bb58e",
      "e758f538255c4e8d93e4dbdc59a49d98"
     ]
    },
    "id": "me1mR6nLf6D1",
    "outputId": "d6a6ffd1-8131-4f7f-c920-83f6f1357293"
   },
   "outputs": [],
   "source": [
    "# checking full data report using profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(cluster_data, title='Dataset Report')\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsEjE3xtgM08"
   },
   "outputs": [],
   "source": [
    "# Saving the data\n",
    "# cluster_data.to_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/cluster_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SL0L82h3jybO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Xgs2UWoufOzJ",
    "outputId": "632a244d-060d-4ba0-d78f-625157e7c524"
   },
   "outputs": [],
   "source": [
    "cluster_data = pd.read_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/cluster_data.csv\")\n",
    "cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdKCtKJ0hN2Z"
   },
   "source": [
    "### Balancing dataset based on AQI:\n",
    "As you can see our dataset is not balanced based on AQI and this will riase biase on our model during training specially for classification models. So to handle them we need to balance our dataset.<br>\n",
    "One best method for handling is SMOTEENN method. this method is more accurate then normal SMOTE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlUWSeAdhCeZ"
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "\n",
    "def balance(data):\n",
    "    # Separate features and target\n",
    "    X = data.drop('aqi', axis=1)  # Feature columns\n",
    "    y = data['aqi']  # Target column\n",
    "\n",
    "    # Check the class distribution before SMOTE\n",
    "    print(\"Original class distribution:\", Counter(y))\n",
    "\n",
    "    # Apply SMOTE to balance the data\n",
    "    smote_enn = SMOTEENN(random_state=42)\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "\n",
    "    # Check the class distribution after SMOTE\n",
    "    print(\"Resampled class distribution:\", Counter(y_resampled))\n",
    "\n",
    "    # Combine resampled X and y into a new DataFrame (optional)\n",
    "    balanced_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    balanced_data['aqi'] = y_resampled\n",
    "    return balanced_data\n",
    "\n",
    "balanced_data = balance(cluster_data)\n",
    "balanced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVR5uyKDiyqU"
   },
   "source": [
    "### Lag & Rolling:\n",
    "For creating air quality prediction we need to lag and roll main features that based on these features and data the model be able to predict air quality for 1,3,6,24,48 and 72 hours later or future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "BZfdMmq8fXxn",
    "outputId": "c9a4a3de-3277-4251-917c-bf700d2cd712"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Lag and Rolling is used to create trend and seasonality on dataset for regression model\n",
    "  and as we do not have much amount of data and specially historical data we need to handle it\n",
    "  using roll and lag and create features for prediction of specific time perioud.\n",
    "\"\"\"\n",
    "def create_lag_and_rolling_features(df, num_cols, time_cols, lags, rolling_windows):\n",
    "    # Initialize DataFrames for lagged and rolling features\n",
    "    lagged_features = pd.DataFrame()\n",
    "    rolling_features = pd.DataFrame()\n",
    "\n",
    "    # Create lag and rolling features for each numeric column\n",
    "    for col in num_cols:\n",
    "        # Lag features\n",
    "        for lag in lags:\n",
    "            lagged_features[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "\n",
    "        # Rolling features\n",
    "        for window in rolling_windows:\n",
    "            rolling_features[f\"{col}_roll_mean_{window}\"] = df[col].rolling(window=window).mean()\n",
    "            rolling_features[f\"{col}_roll_std_{window}\"] = df[col].rolling(window=window).std()\n",
    "\n",
    "    # Combine lag and rolling features\n",
    "    combined_features = pd.concat([lagged_features, rolling_features], axis=1)\n",
    "\n",
    "    # Combine with original time features\n",
    "    final_df = pd.concat([df[time_cols + num_cols], combined_features], axis=1)\n",
    "\n",
    "    # Fill missing values from lag and rolling operations using forward fill\n",
    "    final_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Drop remaining NaNs (e.g., at the start of the dataset where no lag/rolling data exists)\n",
    "    final_df.dropna(inplace=True)\n",
    "\n",
    "    # Reset index for a clean dataset\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_cols = ['co', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'aqi']\n",
    "time_cols = ['year', 'month', 'day', 'hour', 'dayofweek', 'region_id']\n",
    "lags = [1, 3, 6, 24, 48, 72]\n",
    "rolling_windows = [3, 6, 24]\n",
    "\n",
    "# Call the function\n",
    "processed_data = create_lag_and_rolling_features(balanced_data, num_cols, time_cols, lags, rolling_windows)\n",
    "\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lofdu-xWfby-",
    "outputId": "193d30d7-fea4-4aba-acfc-3246582b553a"
   },
   "outputs": [],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qRSnOKzipvw",
    "outputId": "83c802ea-d2a2-4afa-8d03-93ea7705196e"
   },
   "outputs": [],
   "source": [
    "for row, col in processed_data.isna().sum().items():\n",
    "  print(row, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKvCQy5ABcDp",
    "outputId": "031abaad-918e-4ed3-9834-ce063c52801b"
   },
   "outputs": [],
   "source": [
    "# checking after lag and rolling some features to be in its type\n",
    "for x in ['year', 'month', 'day', 'hour', 'dayofweek', 'aqi']:\n",
    "  print(processed_data[x].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "BvLt_7k2nFhe",
    "outputId": "a3757a45-8a92-450b-c135-bace3aafea9a"
   },
   "outputs": [],
   "source": [
    "# Statistical analysis of new data\n",
    "processed_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPXzfh2BvkYh"
   },
   "source": [
    "### Missing values After Creating Lag and Rolling.\n",
    "We are going to fill using KNN and lag based imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rco_6Jsov2V"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# 1. Define a function to apply KNN imputation to columns dynamically\n",
    "def apply_knn_imputation(df, threshold=0.3):\n",
    "    knn_columns = df.columns[df.isna().mean() < threshold]  # Columns with <30% missing values\n",
    "    knn_data = df[knn_columns]\n",
    "\n",
    "    # Initialize KNN imputer\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # Apply KNN imputation\n",
    "    knn_imputed_data = pd.DataFrame(knn_imputer.fit_transform(knn_data), columns=knn_columns)\n",
    "\n",
    "    # Replace the original columns with the KNN imputed ones\n",
    "    df[knn_columns] = knn_imputed_data\n",
    "    return df\n",
    "\n",
    "# 2. Define a function to apply lag-based imputation for time-dependent features\n",
    "def lag_imputation(df, threshold=0.3, lag_hours=1):\n",
    "    lag_columns = [col for col in df.columns if \"lag\" in col and df[col].isna().mean() > threshold]\n",
    "    for col in lag_columns:\n",
    "        df[col] = df[col].fillna(df[col].shift(lag_hours))  # Use previous time step for imputation\n",
    "    return df\n",
    "\n",
    "# 3. Define a function to apply rolling mean imputation\n",
    "def rolling_imputation(df, threshold=0.3, window=3):\n",
    "    rolling_columns = [col for col in df.columns if \"rolling\" in col and df[col].isna().mean() > threshold]\n",
    "    for col in rolling_columns:\n",
    "        df[col] = df[col].fillna(df[col].rolling(window=window, min_periods=1).mean())  # Rolling mean\n",
    "    return df\n",
    "\n",
    "# Apply KNN imputation (for columns with less than 30% missing data)\n",
    "data = apply_knn_imputation(processed_data)\n",
    "\n",
    "# Apply Lag-based imputation (for columns with lag in the name and >30% missing)\n",
    "data = lag_imputation(data)\n",
    "\n",
    "# Apply Rolling mean imputation (for columns with rolling in the name and >30% missing)\n",
    "data = rolling_imputation(data)\n",
    "\n",
    "\n",
    "assert data.isna().sum().sum() == 0, \"There are still missing values in the dataset.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERyWhemXvMGe",
    "outputId": "6e7c6a50-c09d-4d08-a056-7a30b4c25711"
   },
   "outputs": [],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLYrlSi7CkWF",
    "outputId": "19ed0b05-77ac-4805-95aa-1e516c13ec6e"
   },
   "outputs": [],
   "source": [
    "# checking after lag and rolling some features to be in its type\n",
    "for x in ['year', 'month', 'day', 'hour', 'dayofweek', 'aqi']:\n",
    "  print(data[x].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6N9X1tLvetX"
   },
   "outputs": [],
   "source": [
    "# data.to_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JVEw9d8wnOo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "HpUCrFITbO99",
    "outputId": "ba7e3fbf-a817-4f3f-e1e3-3cdce87d798d"
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv(\"added the cleaned dataset\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lQdEOZzhkVSn",
    "outputId": "422917ac-571b-467e-c21e-498b7fc401d2"
   },
   "outputs": [],
   "source": [
    "# ploting the regression of time series data\n",
    "for feature in data.columns:\n",
    "  if feature in ['aqi_lag_24', 'aqi_lag_48', 'aqi_lag_72', 'aqi_lag_1', 'aqi_lag_3', 'aqi_lag_6']:\n",
    "    plt.plot(data[feature])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjTZIKapb4fg"
   },
   "source": [
    "### Ready Time to select and train Our Model 😉😎\n",
    "But Before Selecting our model we are going to encode the target variables for classification and spliting the data.<br>\n",
    "In this some cells we are going to train 3 different model.\n",
    "* **CNN**\n",
    "* **LSTM**\n",
    "* **Transformer**\n",
    "\n",
    "Time to Enjoy.🎉😎😎😎😎😎😎😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "_3eboF-obvtJ",
    "outputId": "6916c33b-7968-454f-f5a8-e3fe477d2eef"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YEwX2nujdkg"
   },
   "source": [
    "### Encoding.\n",
    "Using OneHotEncoder to encode the aqi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ziiGKyljR2d"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def encode_classification_targets(classification_targets):\n",
    "    \"\"\"\n",
    "    Encodes classification targets using one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "        classification_targets (Series or ndarray): Classification target labels.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: One-hot encoded classification targets.\n",
    "    \"\"\"\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_targets = encoder.fit_transform(classification_targets.values.reshape(-1, 1))\n",
    "    return torch.tensor(encoded_targets, dtype=torch.float32), encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReNOhp90jGgd"
   },
   "source": [
    "### Spliting Data:\n",
    "Splite data into train, val and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqbGbHmTygzK"
   },
   "outputs": [],
   "source": [
    "def convert_to_tensors(features, regression_targets):\n",
    "    \"\"\"\n",
    "    Converts features and regression targets to PyTorch tensors.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: PyTorch tensors for features and regression targets.\n",
    "    \"\"\"\n",
    "    features_tensor = torch.tensor(features.values, dtype=torch.float32)\n",
    "    regression_tensor = torch.tensor(regression_targets.values, dtype=torch.float32)\n",
    "    return features_tensor, regression_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDew9jNkpuS3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(features, regression_targets, classification_features, classification_targets,\n",
    "               test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets for both regression and classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "        features (Tensor): Features tensor for regression.\n",
    "        regression_targets (Tensor): Regression target tensor.\n",
    "        classification_features (Tensor): Main features for classification.\n",
    "        classification_targets (Tensor): Encoded classification targets.\n",
    "        test_size (float): Proportion of the data to use for testing.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Training and testing sets for regression and classification tasks.\n",
    "    \"\"\"\n",
    "    # Regression split\n",
    "    X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "        features, regression_targets, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Classification split\n",
    "    X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "        classification_features, classification_targets, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (X_reg_train, X_reg_test, y_reg_train, y_reg_test,\n",
    "            X_class_train, X_class_test, y_class_train, y_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y_88Z6p1Uj8"
   },
   "outputs": [],
   "source": [
    "def select_classification_features(data, feature_columns):\n",
    "    \"\"\"\n",
    "    Selects specific features for classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Input data.\n",
    "        feature_columns (list): List of column names to select for classification.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Selected features for classification.\n",
    "    \"\"\"\n",
    "    return data[feature_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "id": "3gGcenqwyrjY",
    "outputId": "b2b48e15-3e41-4900-a4b4-05a8c33cdaa3"
   },
   "outputs": [],
   "source": [
    "# Regression features and targets (unchanged)\n",
    "features = data[['aqi_lag_24', 'aqi_lag_48', 'aqi_lag_72', 'aqi_lag_1', 'aqi_lag_3', 'aqi_lag_6']]\n",
    "regression_targets = data[['aqi_shift_24h', 'aqi_shift_48h', 'aqi_shift_72h']]\n",
    "\n",
    "# Classification features (main features) and target\n",
    "main_features = ['co', 'no2', 'o3', 'so2', 'pm2_5', 'pm10']\n",
    "classification_features = select_classification_features(data, main_features)\n",
    "classification_targets = data['aqi']\n",
    "\n",
    "# Encode classification targets\n",
    "classification_tensor, encoder = encode_classification_targets(classification_targets)\n",
    "\n",
    "# Convert features and regression targets to tensors\n",
    "features_tensor, regression_tensor = convert_to_tensors(features, regression_targets)\n",
    "classification_features_tensor = torch.tensor(classification_features.values, dtype=torch.float32)\n",
    "\n",
    "# Split data\n",
    "(X_reg_train, X_reg_test, y_reg_train, y_reg_test,\n",
    " X_class_train, X_class_test, y_class_train, y_class_test) = split_data(\n",
    "    features_tensor, regression_tensor, classification_features_tensor, classification_tensor,\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FALnn9ZVgZl1"
   },
   "source": [
    "### Finally time to Train our First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LhUSsK73tiy",
    "outputId": "fae9f83f-c132-4951-93b4-a2ad8053fb4e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Assuming input tensors are already defined:\n",
    "# X_reg_train, y_reg_train, y_class_train, X_reg_test, y_reg_test, y_class_test\n",
    "\n",
    "# Define the Multi-Task CNN Model\n",
    "class MultiTaskAQIModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, output_size):\n",
    "        super(MultiTaskAQIModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.shared_fc = nn.Linear(32 * input_size, 128)\n",
    "\n",
    "        # Regression head\n",
    "        self.reg_fc = nn.Linear(128, output_size)\n",
    "\n",
    "        # Classification head\n",
    "        self.class_fc = nn.Linear(128, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for Conv1D\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.shared_fc(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Separate outputs\n",
    "        regression_output = self.reg_fc(x)\n",
    "        classification_output = self.class_fc(x)\n",
    "\n",
    "        return regression_output, classification_output\n",
    "\n",
    "# Training Function\n",
    "def train_multi_task_model(X_train, y_reg_train, y_class_train, X_test, y_reg_test, y_class_test,\n",
    "                           input_size, num_classes, output_size, epochs, batch_size, learning_rate, save_model_path):\n",
    "    # Convert to PyTorch DataLoader\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                   torch.tensor(y_reg_train, dtype=torch.float32),\n",
    "                                   torch.tensor(y_class_train, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                  torch.tensor(y_reg_test, dtype=torch.float32),\n",
    "                                  torch.tensor(y_class_test, dtype=torch.float32))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, optimizer, and loss functions\n",
    "    model = MultiTaskAQIModel(input_size=input_size, num_classes=num_classes, output_size=output_size)\n",
    "    regression_criterion = nn.MSELoss()\n",
    "    classification_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training Loop\n",
    "    train_history = {'reg_loss': [], 'class_loss': []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_reg_loss, train_class_loss = 0.0, 0.0\n",
    "\n",
    "        for X_batch, y_reg_batch, y_class_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            reg_output, class_output = model(X_batch)\n",
    "\n",
    "            # Compute losses\n",
    "            reg_loss = regression_criterion(reg_output, y_reg_batch)\n",
    "            class_loss = classification_criterion(class_output, y_class_batch.argmax(dim=1))\n",
    "            loss = reg_loss + class_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_reg_loss += reg_loss.item()\n",
    "            train_class_loss += class_loss.item()\n",
    "\n",
    "        # Record losses\n",
    "        train_history['reg_loss'].append(train_reg_loss / len(train_loader))\n",
    "        train_history['class_loss'].append(train_class_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Regression Loss: {train_history['reg_loss'][-1]}, Classification Loss: {train_history['class_loss'][-1]}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    print(f\"Model saved at {save_model_path}!\")\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_loader': train_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'train_history': train_history\n",
    "    }\n",
    "\n",
    "# Variable Declarations\n",
    "input_size = X_reg_train.shape[1]  # Number of features in regression input\n",
    "num_classes = y_class_train.shape[1]  # Number of classes for classification\n",
    "output_size = y_reg_train.shape[1]  # Number of regression outputs\n",
    "\n",
    "# Train the model\n",
    "results = train_multi_task_model(\n",
    "    X_train=X_reg_train, y_reg_train=y_reg_train, y_class_train=y_class_train,\n",
    "    X_test=X_reg_test, y_reg_test=y_reg_test, y_class_test=y_class_test,\n",
    "    input_size=input_size, num_classes=num_classes, output_size=output_size,\n",
    "    epochs=10, batch_size=64, learning_rate=0.001,\n",
    "    save_model_path=\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/multi_task_aqi_model.pth\"\n",
    ")\n",
    "\n",
    "# Extract variables for evaluation and visualization\n",
    "trained_model = results['model']\n",
    "train_loader = results['train_loader']\n",
    "test_loader = results['test_loader']\n",
    "train_history = results['train_history']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "YPu666U_5w0Z",
    "outputId": "539ff5fa-b8ce-4d1f-c380-7915a26b3996"
   },
   "outputs": [],
   "source": [
    "# Plot the losses over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_history['reg_loss'], label=\"Regression Loss\")\n",
    "plt.plot(train_history['class_loss'], label=\"Classification Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3ik1RDM7FjJ",
    "outputId": "ec3ab8f9-bcbd-47fe-f6db-7150cb57d3b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    y_reg_true, y_reg_pred = [], []\n",
    "    y_class_true, y_class_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_reg_batch, y_class_batch in test_loader:\n",
    "            reg_output, class_output = model(X_batch)\n",
    "\n",
    "            # Regression predictions\n",
    "            y_reg_true.append(y_reg_batch.numpy())\n",
    "            y_reg_pred.append(reg_output.numpy())\n",
    "\n",
    "            # Classification predictions\n",
    "            y_class_true.append(y_class_batch.numpy())\n",
    "            y_class_pred.append(class_output.numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    y_reg_true = np.concatenate(y_reg_true, axis=0)\n",
    "    y_reg_pred = np.concatenate(y_reg_pred, axis=0)\n",
    "    y_class_true = np.argmax(np.concatenate(y_class_true, axis=0), axis=1)  # One-hot to class index\n",
    "    y_class_pred = np.argmax(np.concatenate(y_class_pred, axis=0), axis=1)  # Class probabilities to index\n",
    "\n",
    "    # Classification Metrics\n",
    "    classification_acc = accuracy_score(y_class_true, y_class_pred)\n",
    "    classification_report_str = classification_report(y_class_true, y_class_pred)\n",
    "\n",
    "    # Regression Metrics\n",
    "    mse = mean_squared_error(y_reg_true, y_reg_pred)\n",
    "    mae = mean_absolute_error(y_reg_true, y_reg_pred)\n",
    "    r2 = r2_score(y_reg_true, y_reg_pred)\n",
    "\n",
    "    # Print Metrics\n",
    "    print(\"Classification Metrics:\")\n",
    "    print(f\"Accuracy: {classification_acc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report_str)\n",
    "\n",
    "    print(\"\\nRegression Metrics:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R-Squared (R²): {r2:.4f}\")\n",
    "\n",
    "    # Return metrics if needed for further processing\n",
    "    return {\n",
    "        'classification': {\n",
    "            'accuracy': classification_acc,\n",
    "            'report': classification_report_str,\n",
    "        },\n",
    "        'regression': {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Evaluate the trained model on the test data\n",
    "metrics = evaluate_model(trained_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vq69XZwd1tk",
    "outputId": "268e339a-1a3a-49be-c299-45a20128e32e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "features = data[['aqi_shift_24h', 'aqi_shift_48h', 'aqi_shift_72h', 'aqi_lag_1h', 'aqi_lag_3h', 'aqi_lag_6h']]\n",
    "regression_targets = data[['aqi_shift_24h', 'aqi_shift_48h', 'aqi_shift_72h']]\n",
    "classification_targets = data['aqi']  # AQI classification (1 to 5)\n",
    "\n",
    "# One-hot encode classification targets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "classification_targets = encoder.fit_transform(classification_targets.values.reshape(-1, 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "features_tensor = torch.tensor(features.values, dtype=torch.float32)\n",
    "regression_tensor = torch.tensor(regression_targets.values, dtype=torch.float32)\n",
    "classification_tensor = torch.tensor(classification_targets, dtype=torch.float32)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_reg_train, y_reg_test, y_class_train, y_class_test = train_test_split(\n",
    "    features_tensor, regression_tensor, classification_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch DataLoader\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_reg_train, y_class_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_reg_test, y_class_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the Multi-Task CNN Model\n",
    "class MultiTaskAQIModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, output_size):\n",
    "        super(MultiTaskAQIModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.shared_fc = nn.Linear(32 * input_size, 128)\n",
    "\n",
    "        # Regression head\n",
    "        self.reg_fc = nn.Linear(128, output_size)\n",
    "\n",
    "        # Classification head\n",
    "        self.class_fc = nn.Linear(128, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for Conv1D\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.shared_fc(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Separate outputs\n",
    "        regression_output = self.reg_fc(x)\n",
    "        classification_output = self.class_fc(x)\n",
    "\n",
    "        return regression_output, classification_output\n",
    "\n",
    "# Initialize the model, optimizer, and loss functions\n",
    "input_size = features_tensor.shape[1]\n",
    "num_classes = classification_targets.shape[1]\n",
    "output_size = regression_targets.shape[1]\n",
    "\n",
    "model = MultiTaskAQIModel(input_size=input_size, num_classes=num_classes, output_size=output_size)\n",
    "\n",
    "regression_criterion = nn.MSELoss()\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_reg_loss, train_class_loss = 0.0, 0.0\n",
    "    for X_batch, y_reg_batch, y_class_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reg_output, class_output = model(X_batch)\n",
    "\n",
    "        # Compute losses\n",
    "        reg_loss = regression_criterion(reg_output, y_reg_batch)\n",
    "        class_loss = classification_criterion(class_output, y_class_batch.argmax(dim=1))\n",
    "\n",
    "        loss = reg_loss + class_loss  # Combined loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_reg_loss += reg_loss.item()\n",
    "        train_class_loss += class_loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Regression Loss: {train_reg_loss / len(train_loader)}, Classification Loss: {train_class_loss / len(train_loader)}\")\n",
    "\n",
    "# Save the model\n",
    "# torch.save(model.state_dict(), \"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/multi_task_aqi_model.pth\")\n",
    "# print(\"Model saved!\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "reg_preds, class_preds, reg_true, class_true = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_reg_batch, y_class_batch in test_loader:\n",
    "        reg_output, class_output = model(X_batch)\n",
    "        reg_preds.append(reg_output)\n",
    "        class_preds.append(class_output)\n",
    "        reg_true.append(y_reg_batch)\n",
    "        class_true.append(y_class_batch)\n",
    "\n",
    "reg_preds = torch.cat(reg_preds).numpy()\n",
    "class_preds = torch.cat(class_preds).argmax(dim=1).numpy()\n",
    "reg_true = torch.cat(reg_true).numpy()\n",
    "class_true = torch.cat(class_true).argmax(dim=1).numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score\n",
    "\n",
    "# Regression metrics\n",
    "for i, horizon in enumerate(['24h', '48h', '72h']):\n",
    "    rmse = np.sqrt(mean_squared_error(reg_true[:, i], reg_preds[:, i]))\n",
    "    r2 = r2_score(reg_true[:, i], reg_preds[:, i])\n",
    "    print(f\"{horizon} Regression - RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
    "\n",
    "# Classification accuracy\n",
    "accuracy = accuracy_score(class_true, class_preds)\n",
    "print(f\"Classification Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "BfM5V6LfnBQQ",
    "outputId": "9648b393-4a1b-42e0-dbad-720340be6725"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test.numpy())\n",
    "\n",
    "# 2. Apply PCA for dimensionality reduction to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_test_pca = pca.fit_transform(X_test_scaled)\n",
    "\n",
    "# 3. Create a meshgrid in the 2D PCA-transformed feature space\n",
    "x_min, x_max = X_test_pca[:, 0].min() - 1, X_test_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_test_pca[:, 1].min() - 1, X_test_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# 4. Map the meshgrid back to the original feature space\n",
    "grid_points_pca = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_original = pca.inverse_transform(grid_points_pca)\n",
    "grid_points_tensor = torch.tensor(scaler.inverse_transform(grid_points_original), dtype=torch.float32)\n",
    "\n",
    "# 5. Predict the classification output for each point in the meshgrid\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, grid_preds = model(grid_points_tensor)\n",
    "grid_preds = grid_preds.argmax(dim=1).numpy().reshape(xx.shape)\n",
    "\n",
    "# 6. Plot the decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, grid_preds, alpha=0.8, cmap=plt.cm.Paired)\n",
    "plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=class_true, edgecolor='k', cmap=plt.cm.Paired)\n",
    "plt.title(\"Decision Boundary for Classification\")\n",
    "plt.xlabel(\"PCA Feature 1\")\n",
    "plt.ylabel(\"PCA Feature 2\")\n",
    "plt.colorbar(label=\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMPXy1iPk70T",
    "outputId": "a78d04c9-852a-4303-d1df-b7058642fdc2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Preprocess the Data\n",
    "X = data[['co', 'no2', 'o3', 'so2', 'pm2_5', 'pm10']]\n",
    "y = data['aqi']\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Train SVM\n",
    "svm_model = SVC(kernel='rbf', random_state=42)  # Radial basis function kernel\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Step 4: Train Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate Models\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_predictions))\n",
    "\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1r-iB4H8gi2g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3B1eTE1hgizd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCzvnUq7giw8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISIetv5sh95J"
   },
   "source": [
    "### Checking Classification on different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TTiGee7guBh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "7G0pqj83gt-E",
    "outputId": "c5e6e316-0da9-43ec-8c74-73042a527fd7"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/processed_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm6g6DyFgjkK"
   },
   "source": [
    "### Testing different Activation function on CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "W0Ydf8VFCMhF",
    "outputId": "4652adcd-244f-4b27-a3a6-f53d04fa39be"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Function to encode ProtocolName using one-hot encoding and rename the feature to Label\n",
    "def encode_protocol_name(dataset):\n",
    "    # Initialize the OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    # Extract the 'ProtocolName' column and reshape it to 2D array\n",
    "    protocol_name_column = dataset[['aqi']]\n",
    "\n",
    "    # Fit and transform the 'ProtocolName' column\n",
    "    one_hot_encoded = encoder.fit_transform(protocol_name_column)\n",
    "\n",
    "    # Convert the one-hot encoded array into a DataFrame\n",
    "    one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(['aqi']))\n",
    "\n",
    "    # Drop the original ProtocolName column\n",
    "    dataset.drop('aqi', axis=1, inplace=True)\n",
    "\n",
    "    # Concatenate the original dataset with the one-hot encoded DataFrame\n",
    "    dataset = pd.concat([data, one_hot_encoded_df], axis=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "new_data = encode_protocol_name(data)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpMAnKUfCfzE"
   },
   "outputs": [],
   "source": [
    "# importing train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split function\n",
    "def split_data(dataset):\n",
    "    dataset = dataset.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    one_hot_columns = [col for col in dataset.columns if col in ['aqi_1.0',\t'aqi_2.0',\t'aqi_3.0',\t'aqi_4.0',\t'aqi_5.0']]\n",
    "\n",
    "    # Split the dataset into X (features) and y (labels)\n",
    "    x = dataset[['co', 'no2', 'o3', 'so2', 'pm2_5', 'pm10']]\n",
    "    y = dataset[one_hot_columns]\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.7, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jh7bM-kD-Bv"
   },
   "outputs": [],
   "source": [
    "# convert Pandas DataFrame into Tensorflow Tensor\n",
    "import tensorflow as tf\n",
    "def dataframe_to_tensor():\n",
    "  '''\n",
    "  This function is going to convert Pandas DataFrame into TensorFlow Tensor\n",
    "  and then Reshape them from 2D to 3D for model fitting.\n",
    "  And at last change them to tensorflow dataset and shuffle them for better model operation.\n",
    "  '''\n",
    "  # set random seed\n",
    "  tf.random.set_seed(1)\n",
    "\n",
    "  # train validate and test the dataset first using scikit-learn then convert them into tensor\n",
    "  x_train, x_val, x_test, y_train, y_val, y_test = split_data(new_data)\n",
    "\n",
    "  # DataFrame to TensorFlow Tensor\n",
    "  x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "  x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
    "  x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "  y_train = tf.convert_to_tensor(y_train, dtype=tf.int16)\n",
    "  y_val = tf.convert_to_tensor(y_val, dtype=tf.int16)\n",
    "  y_test = tf.convert_to_tensor(y_test, dtype=tf.int16)\n",
    "\n",
    "\n",
    "  # reshape the tensor from 2D to 3D\n",
    "  x_train = tf.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "  x_val = tf.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1))\n",
    "  x_test = tf.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "  # Convert to tf.data.Dataset\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "  val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "  # Shuffle and batch the datasets\n",
    "  batch_size = 32  # Try reducing this further if needed\n",
    "  train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpM-KpgnEtKc"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset, x_test, y_test = dataframe_to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xA0j-F2VrQBu",
    "outputId": "bcf0293b-1fc0-4488-dedc-5b1cd593e48b"
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add Conv1D layers with padding to avoid negative dimensions\n",
    "    model.add(layers.Conv1D(32, 4, padding='same', activation='tanh', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    # model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # model.add(layers.Conv1D(64, 4, padding='same', activation='gelu'))\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.MaxPooling1D(2))\n",
    "    # # model.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "    # Flatten the output and add Dense layers for classification\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='tanh'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # Multi-class classification, use softmax activation\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (6, 1)  # Update this to match your dataset's input shape\n",
    "num_classes = 5       # Update this to match the number of classes in your dataset\n",
    "\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',  # Use sparse_categorical_crossentropy if labels are not one-hot encoded\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Assume train_dataset, val_dataset, and test_dataset are already created and preprocessed\n",
    "# timeout\n",
    "start = time.time()\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=15,\n",
    "                    validation_data=val_dataset)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Save the model\n",
    "# model.save('/content/drive/MyDrive/Final-Project/Saved-models/AppMultiClassification/final_multiclass.h5')\n",
    "\n",
    "# # Save the training history\n",
    "# with open('/content/drive/MyDrive/Final-Project/Saved-models/AppMultiClassification/final_multiclass.pkl', 'wb') as file:\n",
    "#     pickle.dump(history.history, file)\n",
    "\n",
    "duration = end - start\n",
    "\n",
    "print(f'Training time: {duration:.2f} seconds')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "TPn_2QjQsF3D",
    "outputId": "a51804a8-0073-4853-f459-c217d546108e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Standardize the features\n",
    "scaler = StandardScaler()\n",
    "x_test_scaled = scaler.fit_transform(x_test.numpy().reshape(-1, 6))  # Flatten input for scaling\n",
    "\n",
    "# 2. Apply PCA for dimensionality reduction to 2D\n",
    "pca = PCA(n_components=2)\n",
    "x_test_pca = pca.fit_transform(x_test_scaled)\n",
    "\n",
    "# 3. Create a meshgrid in the 2D PCA-transformed feature space\n",
    "x_min, x_max = x_test_pca[:, 0].min() - 1, x_test_pca[:, 0].max() + 1\n",
    "y_min, y_max = x_test_pca[:, 1].min() - 1, x_test_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# 4. Map the meshgrid back to the original feature space\n",
    "grid_points_pca = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_original = pca.inverse_transform(grid_points_pca)\n",
    "grid_points_scaled = scaler.inverse_transform(grid_points_original)\n",
    "grid_points_tensor = tf.convert_to_tensor(grid_points_scaled, dtype=tf.float32)\n",
    "\n",
    "# 5. Predict the classification output for each point in the meshgrid\n",
    "model.evaluate(x_test, y_test)  # Optional evaluation step\n",
    "grid_preds = model.predict(grid_points_tensor)\n",
    "grid_preds = np.argmax(grid_preds, axis=1).reshape(xx.shape)\n",
    "\n",
    "# 6. Plot the decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, grid_preds, alpha=0.8, cmap=plt.cm.Paired)\n",
    "\n",
    "# Ensure class_true is properly defined\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "    class_true = np.argmax(y_test, axis=1)  # Convert one-hot to class indices\n",
    "else:\n",
    "    class_true = y_test.reshape(-1)\n",
    "\n",
    "plt.scatter(x_test_pca[:, 0], x_test_pca[:, 1], c=class_true, edgecolor='k', cmap=plt.cm.Paired)\n",
    "plt.title(\"Decision Boundary for Classification\")\n",
    "plt.xlabel(\"PCA Feature 1\")\n",
    "plt.ylabel(\"PCA Feature 2\")\n",
    "plt.colorbar(label=\"Predicted Class\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JbiuTT50E2W7",
    "outputId": "4c37f753-d6d0-481d-b68c-02191c55566f"
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add Conv1D layers with padding to avoid negative dimensions\n",
    "    model.add(layers.Conv1D(32, 4, padding='same', activation='tanh', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    # model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv1D(64, 4, padding='same', activation='gelu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "    # model.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "    # Flatten the output and add Dense layers for classification\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='tanh'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    # model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # Multi-class classification, use softmax activation\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (12, 1)  # Update this to match your dataset's input shape\n",
    "num_classes = 5       # Update this to match the number of classes in your dataset\n",
    "\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',  # Use sparse_categorical_crossentropy if labels are not one-hot encoded\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Assume train_dataset, val_dataset, and test_dataset are already created and preprocessed\n",
    "# timeout\n",
    "start = time.time()\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=15,\n",
    "                    validation_data=val_dataset)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Save the model\n",
    "# model.save('/content/drive/MyDrive/Final-Project/Saved-models/AppMultiClassification/final_multiclass.h5')\n",
    "\n",
    "# # Save the training history\n",
    "# with open('/content/drive/MyDrive/Final-Project/Saved-models/AppMultiClassification/final_multiclass.pkl', 'wb') as file:\n",
    "#     pickle.dump(history.history, file)\n",
    "\n",
    "duration = end - start\n",
    "\n",
    "print(f'Training time: {duration:.2f} seconds')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nn2FU2b20LbO",
    "outputId": "ea6ad1f2-36b3-42b0-cdbf-cd5deb3466eb"
   },
   "outputs": [],
   "source": [
    "# Hyperprameter tunning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import numpy as np\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = [\n",
    "    Integer(16, 128, name='conv1_filters'),  # Number of filters in the first Conv1D layer\n",
    "    Integer(16, 128, name='conv2_filters'),  # Number of filters in the second Conv1D layer\n",
    "    Integer(32, 256, name='dense_units'),    # Number of units in Dense layer\n",
    "    Real(1e-4, 1e-2, prior='log-uniform', name='learning_rate'),  # Learning rate\n",
    "    Categorical(['tanh', 'relu'], name='activation'),  # Activation function\n",
    "]\n",
    "\n",
    "# Objective function for Bayesian Optimization\n",
    "@use_named_args(search_space)\n",
    "def objective_function(**params):\n",
    "    # Build the CNN model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(params['conv1_filters'], kernel_size=4, activation=params['activation'], padding='same', input_shape=(6, 1)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    model.add(layers.Conv1D(params['conv2_filters'], kernel_size=4, activation=params['activation'], padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(params['dense_units'], activation=params['activation']))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(5, activation='softmax'))  # Assume 5 classes for multi-class classification\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model on a subset of data\n",
    "    history = model.fit(train_dataset, epochs=5, validation_data=val_dataset, verbose=0)  # Use fewer epochs for faster tuning\n",
    "\n",
    "    # Return validation accuracy (objective to maximize)\n",
    "    val_accuracy = max(history.history['val_accuracy'])\n",
    "    return -val_accuracy  # Negative because gp_minimize minimizes the objective function\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "result = gp_minimize(objective_function, search_space, n_calls=20, random_state=42)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"Conv1 Filters: {result.x[0]}\")\n",
    "print(f\"Conv2 Filters: {result.x[1]}\")\n",
    "print(f\"Dense Units: {result.x[2]}\")\n",
    "print(f\"Learning Rate: {result.x[3]}\")\n",
    "print(f\"Activation: {result.x[4]}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_params = {\n",
    "    'conv1_filters': result.x[0],\n",
    "    'conv2_filters': result.x[1],\n",
    "    'dense_units': result.x[2],\n",
    "    'learning_rate': result.x[3],\n",
    "    'activation': result.x[4]\n",
    "}\n",
    "\n",
    "final_model = models.Sequential()\n",
    "final_model.add(layers.Conv1D(best_params['conv1_filters'], kernel_size=4, activation=best_params['activation'], padding='same', input_shape=(6, 1)))\n",
    "final_model.add(layers.BatchNormalization())\n",
    "final_model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "final_model.add(layers.Conv1D(best_params['conv2_filters'], kernel_size=4, activation=best_params['activation'], padding='same'))\n",
    "final_model.add(layers.BatchNormalization())\n",
    "final_model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "final_model.add(layers.Flatten())\n",
    "final_model.add(layers.Dense(best_params['dense_units'], activation=best_params['activation']))\n",
    "final_model.add(layers.BatchNormalization())\n",
    "final_model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "final_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the final model\n",
    "history = final_model.fit(train_dataset, epochs=15, validation_data=val_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = final_model.evaluate(test_dataset)\n",
    "print(\"Test accuracy with optimized hyperparameters:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MUgoPvg95za9",
    "outputId": "fd8dc8f8-2cb9-41d1-fae4-35a7cdfab1d1"
   },
   "outputs": [],
   "source": [
    "# Best Hyperparameter after tunning lets find out\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the optimized CNN model\n",
    "def create_optimized_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First Conv1D layer with optimized number of filters and activation function\n",
    "    model.add(layers.Conv1D(105, kernel_size=4, activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    model.add(layers.Conv1D(58, kernel_size=4, activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Flatten the output and add Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # Multi-class classification\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (6, 1)  # Update to match your dataset\n",
    "num_classes = 5       # Update to match your dataset\n",
    "\n",
    "# Create the model\n",
    "model = create_optimized_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model with the optimized learning rate\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.00041633970544941064)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Assume train_dataset, val_dataset, and test_dataset are already prepared\n",
    "# Train the model\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=15,\n",
    "                    validation_data=val_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(\"Test accuracy with optimized hyperparameters:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yV1_1CoO-2q6",
    "outputId": "2aec07cf-d68c-448d-bbe2-3d2f249a27b7"
   },
   "outputs": [],
   "source": [
    "# Adding two more convolutional layer and lets check\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # First Conv1D layer\n",
    "    model.add(layers.Conv1D(32, 4, padding='same', activation='tanh', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    # Second Conv1D layer\n",
    "    model.add(layers.Conv1D(64, 4, padding='same', activation='gelu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    # Third Conv1D layer\n",
    "    model.add(layers.Conv1D(128, 4, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(2))  # Use pooling here if the spatial dimensions allow.\n",
    "\n",
    "    # Fourth Conv1D layer\n",
    "    model.add(layers.Conv1D(256, 4, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Use Global Pooling instead of MaxPooling to avoid spatial collapse\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "    # Flatten the output and add Dense layers for classification\n",
    "    model.add(layers.Dense(64, activation='tanh'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # Multi-class classification\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (12, 1)  # Ensure this matches your dataset's input shape\n",
    "num_classes = 5        # Update this to match the number of classes in your dataset\n",
    "\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Assume train_dataset, val_dataset, and test_dataset are already created and preprocessed\n",
    "# Measure training time\n",
    "start = time.time()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=15,\n",
    "                    validation_data=val_dataset)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Save the model and training history (optional)\n",
    "# model.save('/content/drive/MyDrive/Final-Project/Saved-models/AppMultiClassification/final_multiclass.h5')\n",
    "# with open('/content/drive/MyDrive/Final-Project/Saved-models/AppMultiClassification/final_multiclass.pkl', 'wb') as file:\n",
    "#     pickle.dump(history.history, file)\n",
    "\n",
    "duration = end - start\n",
    "print(f'Training time: {duration:.2f} seconds')\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "nqN3LyfgE4WU",
    "outputId": "e6121240-7f02-446b-9fdb-51920993dd81"
   },
   "outputs": [],
   "source": [
    "# new lastm model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=((12,1)), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # 5 classes in the target variable\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset,validation_data=val_dataset, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yNb-exOoqbb",
    "outputId": "c4be4831-695a-445f-dafa-f3cff460c50a"
   },
   "outputs": [],
   "source": [
    "# LSTM algorithm\n",
    "from tensorflow.keras import layers, models\n",
    "def create_rnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(layers.LSTM(64))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (6, 1)  # dataset's input shape\n",
    "num_classes = 5       # Number of classes in dataset\n",
    "model = create_rnn_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=8)\n",
    "# model.save(\"/kaggle/working/rnn_multi_app.h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEMoaa5HBrJC",
    "outputId": "275801bc-690c-4831-c6f1-539cc4e795fd"
   },
   "outputs": [],
   "source": [
    "# Transformer model\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, Input, MultiHeadAttention, Embedding\n",
    "\n",
    "def create_transformer_model(input_shape, num_classes, num_heads=4, d_model=64, ff_dim=128, dropout_rate=0.1):\n",
    "    # Input\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Embedding Layer\n",
    "    x = layers.Dense(d_model)(inputs)  # Project input to d_model dimensions\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Multi-Head Attention and Feed-Forward\n",
    "    for _ in range(2):  # Add 2 Transformer encoder layers\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        attn_output = Dropout(dropout_rate)(attn_output)\n",
    "        x = layers.add([x, attn_output])  # Residual connection\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        ffn_output = Dense(ff_dim, activation=\"relu\")(x)\n",
    "        ffn_output = Dense(d_model)(ffn_output)\n",
    "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "        x = layers.add([x, ffn_output])  # Residual connection\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Pooling and Output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    # Create Model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (6, 1)  # Input shape (timesteps, features)\n",
    "num_classes = 5       # Number of classes\n",
    "model = create_transformer_model(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=8)\n",
    "\n",
    "# Save the model\n",
    "# model.save(\"/kaggle/working/transformer_multi_app.h5\")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDCmUhUMM22_",
    "outputId": "021dc518-4262-4440-9244-6809e842d796"
   },
   "outputs": [],
   "source": [
    "# Hybrid model (LSTM, CNN and Transformer)\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv1D, MaxPooling1D, LSTM, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "\n",
    "def create_hybrid_model(input_shape, num_classes):\n",
    "\n",
    "    # Shared Input\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Branch 1: LSTM\n",
    "    lstm_branch = LSTM(64, return_sequences=True)(inputs)\n",
    "    lstm_branch = LSTM(64)(lstm_branch)\n",
    "\n",
    "    # Branch 2: CNN\n",
    "    cnn_branch = Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "    cnn_branch = MaxPooling1D(pool_size=2)(cnn_branch)\n",
    "    cnn_branch = Flatten()(cnn_branch)\n",
    "\n",
    "    # Branch 3: Transformer\n",
    "    transformer_branch = Dense(64)(inputs)  # Project to Transformer dimensions\n",
    "    transformer_branch = LayerNormalization(epsilon=1e-6)(transformer_branch)\n",
    "    attn_output = MultiHeadAttention(num_heads=4, key_dim=64)(transformer_branch, transformer_branch)\n",
    "    attn_output = Dropout(0.1)(attn_output)\n",
    "    transformer_branch = layers.add([transformer_branch, attn_output])  # Residual connection\n",
    "    transformer_branch = LayerNormalization(epsilon=1e-6)(transformer_branch)\n",
    "    transformer_branch = GlobalAveragePooling1D()(transformer_branch)\n",
    "\n",
    "    # Concatenate Branches\n",
    "    concatenated = layers.concatenate([lstm_branch, cnn_branch, transformer_branch])\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    x = Dense(128, activation=\"relu\")(concatenated)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output Layers\n",
    "    classification_output = Dense(num_classes, activation=\"softmax\", name=\"classification_output\")(x)\n",
    "    regression_output = Dense(1, name=\"regression_output\")(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=inputs, outputs=[classification_output, regression_output])\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (6, 1)  # Input shape (timesteps, features)\n",
    "num_classes = 5       # Number of classes\n",
    "model = create_hybrid_model(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'classification_output': 'categorical_crossentropy',\n",
    "                    'regression_output': 'mse'},\n",
    "              metrics={'classification_output': 'accuracy',\n",
    "                       'regression_output': 'mse'})\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = model.evaluate(test_dataset)\n",
    "print(\"Test Results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MYCh6iiPvxR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pWcVU6Z1ye9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qY6hj80b1yKU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glrkko9x1ziB"
   },
   "source": [
    "### Result:\n",
    "Based on discription of CNN, LSTM and Transformer and amount of data we have is that we can not go with these models. Even these models are perfect and they can goes will on our data but because of lack of data we can not apply them using Neural Network. as my dataset is well for Trees structure we will try RandomForest and XGBoost to see. Just for classification, for regression we are going to Neural Network LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vv_glTqO1yG6"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, y_train, y_val, y_test = split_data(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KecTguXh3niP",
    "outputId": "76105239-264e-4149-831c-ae3253ed52f0"
   },
   "outputs": [],
   "source": [
    "# import the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# train the model\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "# Score on validation\n",
    "val_score = rf_model.score(x_val, y_val)\n",
    "\n",
    "# predict on test data\n",
    "rf_pred = rf_model.predict(x_test)\n",
    "\n",
    "# print the result\n",
    "print(f\"RandomForestClassifier on Validation data Score Result: {val_score}\")\n",
    "\n",
    "# print prediction on test dataset\n",
    "print(f\"RandomForestClassifier on Test data Prediction Result: {rf_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SroMJQ1h6dtZ"
   },
   "source": [
    "### Wow such a great Result 😍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9rUddBO5Zdu",
    "outputId": "f45e7eb0-8993-4b09-8a56-e04713759b79"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, roc_auc_score, accuracy_score, recall_score\n",
    "\n",
    "# Evaluation function\n",
    "def rf_eval(model, y_true, y_pred, average='weighted'):\n",
    "    # Evaluate metrics\n",
    "    roc = roc_auc_score(y_true, y_pred, average=average)\n",
    "    preci = precision_score(y_true, y_pred, average=average)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Precision Score Result On Test data: {preci}\")\n",
    "    print(f\"Roc_AUC Score Result On Test data: {roc}\")\n",
    "    print(f\"Accuracy Score Result On Test data: {accuracy}\")\n",
    "    print(f\"Recall Score Result On Test data: {recall}\")\n",
    "\n",
    "    return preci, roc, accuracy, recall\n",
    "\n",
    "# Evaluate the model\n",
    "preci, roc, accuracy, recall = rf_eval(rf_model, y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "TS7ok6U48NwK",
    "outputId": "fd02f1fc-24cb-4920-ec19-79f6f31c741e"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def rf_viz(model, y_true, y_pred, preci, roc, accuracy, recall, class_names=None):\n",
    "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Confusion Matrix Visualization\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(conf_matrix, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=axes[0], colorbar=False)\n",
    "    axes[0].set_title(\"Confusion Matrix\", pad=15)  # Add padding to title\n",
    "\n",
    "    # Visualize metrics as a bar chart\n",
    "    metrics = {\n",
    "        \"Precision\": preci,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC-AUC\": roc\n",
    "    }\n",
    "    bars = axes[1].bar(metrics.keys(), metrics.values(), color=['blue', 'green', 'orange', 'red'])\n",
    "    axes[1].set_ylabel(\"Score\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].set_title(\"Model Evaluation Metrics\", pad=20)  # Add padding to title\n",
    "\n",
    "    # Add values on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width() / 2, height + 0.02, f\"{height:.2f}\", ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "class_names = ['Good', 'Fair', 'Moderate', 'Abnormal', 'Dangerous']  # Replace with your actual class names\n",
    "rf_viz(rf_model, y_test, rf_pred, preci, roc, accuracy, recall, class_names=class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdwTCUzJ_koy",
    "outputId": "979f1ab9-86cc-4a93-b9aa-587dce0da5b8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "k = 10\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(rf_model, x_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# Display cross-validation results\n",
    "print(f\"K-Fold Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# Optional: Check on test set to compare\n",
    "test_score = rf_model.score(x_test, y_test)\n",
    "print(f\"RandomForestClassifier Test Score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FSCwSVKHJSy"
   },
   "source": [
    "### Result:\n",
    "All Right We have done it.<br>Time to celebrate.🎉🎉🎉🎉🎉🎉🎉😎\n",
    "<br>Based on Discription of Evaluation Metrics. Our model learned well and need to be saved for real test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5vqo0saHkyL",
    "outputId": "3ae9d21a-c9aa-4d29-e991-5d9099f851de"
   },
   "outputs": [],
   "source": [
    "# Lets save the model\n",
    "# import joblib\n",
    "# joblib.dump(rf_model, \"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/rf_classification_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX11ywD2Jv5c"
   },
   "source": [
    "### Load RF Model:\n",
    "We need to check feature importance Explainable AI and Decision Boundry to make sure model learned well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "KKmssIQSJvXQ",
    "outputId": "6372e006-b9da-47c3-d21d-83c996079bd5"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "rf_model = joblib.load(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/rf_classification_model.joblib\")\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "OhA65JjzJvTH",
    "outputId": "4cb45a07-ced6-412e-c119-74cbbc2ace7b"
   },
   "outputs": [],
   "source": [
    "# RandomForest Feature Importance\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# features Our model trained on\n",
    "features = ['co', 'no2', 'o3', 'so2', 'pm2_5', 'pm10']\n",
    "\n",
    "# Sort the feature importances\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance - Random Forest\", fontsize=16)\n",
    "plt.bar(range(len(features)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(features)), [features[i] for i in indices], rotation=45, fontsize=12)\n",
    "plt.ylabel(\"Importance Score\", fontsize=14)\n",
    "plt.xlabel(\"Features\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlTX_BdUKanW"
   },
   "source": [
    "### XGBoost Model:\n",
    "As we have mentioned Earlier we are going to check XGBoost model also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "MbA8itmDIfZx",
    "outputId": "2cde04f8-843d-41d9-aa2d-f9beb10fde99"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "if len(y_train.shape) > 1:\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Define and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softprob',  # Multi-class classification\n",
    "    num_class=5,  # Number of target classes\n",
    "    eval_metric='mlogloss',  # Evaluation metric\n",
    "    use_label_encoder=False,  # Prevent label encoding warnings\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Validate\n",
    "xgb_val = xgb_model.score(x_val, y_val)\n",
    "print(f\"XGBoost Model Score on Validation data: {xgb_val}\")\n",
    "\n",
    "# Predict on the test set\n",
    "xgb_pred = xgb_model.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, xgb_pred)\n",
    "print(f\"XGBoost Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=['Good', 'Fair', 'Moderate', 'Abnormal', 'Dangerous']))\n",
    "\n",
    "# Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, xgb_pred, display_labels=['Good', 'Fair', 'Moderate', 'Abnormal', 'Dangerous'], cmap=\"Blues\")\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgb9sQ7HMWwQ"
   },
   "source": [
    "### Result:\n",
    "As you have seen our model learned and predict well on the data.<br>\n",
    "So there is no need for further analysis.<br>\n",
    "Just we need to Train Regression Model Using LSTM and Evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqoURuqhia0Y"
   },
   "source": [
    "### Regression Model training and checking up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c3qrwM6MFDs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "DWukt1gYOgm9",
    "outputId": "4a78c146-5b66-4c69-c5d0-0950b1380328"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/processed_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNXcLdCmWgKV"
   },
   "outputs": [],
   "source": [
    "data['aqi_shift_1h'] = data['aqi'].shift(-1)\n",
    "data['aqi_shift_3h'] = data['aqi'].shift(-3)\n",
    "data['aqi_shift_6h'] = data['aqi'].shift(-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZEADRAay2jI"
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZraFhlOu5W3"
   },
   "outputs": [],
   "source": [
    "# Correct non-integer values in the specified columns\n",
    "columns_to_fix = ['aqi_shift_48h', 'aqi_shift_72h']\n",
    "\n",
    "for column in columns_to_fix:\n",
    "    # Round values to the nearest integers and convert to float\n",
    "    data[column] = data[column].round(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1qeOU-GPbr1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "def split_data(dataset, target):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets (80% training, 5% validation, 15% test).\n",
    "    \"\"\"\n",
    "    # first shuffle the data and then split\n",
    "\n",
    "    dataset = shuffle(dataset, random_state=42)\n",
    "    # Select features and target\n",
    "    X = dataset.drop(['aqi_shift_24h', 'aqi_shift_48h', 'aqi_shift_72h','aqi_shift_1h','aqi_shift_3h','aqi_shift_6h', target], axis=1)\n",
    "\n",
    "    # For future predictions, use the shifted columns as the target\n",
    "    y = dataset[['aqi_shift_24h', 'aqi_shift_48h', 'aqi_shift_72h','aqi_shift_1h','aqi_shift_3h','aqi_shift_6h']]\n",
    "\n",
    "    # Split data into training and remaining data (80% training, 20% remaining)\n",
    "    X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Split the remaining data into validation (5%) and test (15%) from the remaining 20%\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=0.75, shuffle=False)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB6koV0JU7VT"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def dataframe_to_tensor_regression(data):\n",
    "    '''\n",
    "    Convert Pandas DataFrame into TensorFlow Tensor for regression tasks,\n",
    "    reshape features for LSTM/CNN models, and prepare tf.data.Dataset.\n",
    "    '''\n",
    "    # Set random seed\n",
    "    tf.random.set_seed(1)\n",
    "\n",
    "    # Train, validate, and test split\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = split_data(data, 'aqi')\n",
    "\n",
    "    # Convert DataFrame to TensorFlow Tensors\n",
    "    x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    x_val = tf.convert_to_tensor(x_val, dtype=tf.float32)\n",
    "    x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)  # Float for regression\n",
    "    y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "    # Reshape features from 2D to 3D\n",
    "    x_train = tf.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_val = tf.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1))\n",
    "    x_test = tf.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # Convert to tf.data.Dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "    # Shuffle and batch the datasets\n",
    "    batch_size = 32\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, x_test, y_test\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, x_test, y_test = dataframe_to_tensor_regression(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPQ1bGk0VYwO",
    "outputId": "fbbdc936-f6b8-432c-eb13-d6492c463c9d"
   },
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCxHu48rQXEj",
    "outputId": "0338671b-118f-4cf5-ff0b-451b6d13e9c5"
   },
   "outputs": [],
   "source": [
    "time_steps = 10\n",
    "\n",
    "def reshape_targets(y, time_steps):\n",
    "    num_samples = y.shape[0] // time_steps  # Ensure it divides evenly\n",
    "    y = y[:num_samples * time_steps]  # Trim extra rows that don't fit\n",
    "    y_reshaped = y.reshape(num_samples, time_steps, -1)  # Reshape to 3D\n",
    "    return y_reshaped\n",
    "\n",
    "y_train_reshaped = reshape_targets(y_train, time_steps)\n",
    "y_val_reshaped = reshape_targets(y_val, time_steps)\n",
    "y_test_reshaped = reshape_targets(y_test, time_steps)\n",
    "\n",
    "print(\"y_train_reshaped shape:\", y_train_reshaped.shape)\n",
    "print(\"y_val_reshaped shape:\", y_val_reshaped.shape)\n",
    "print(\"y_test_reshaped shape:\", y_test_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MC22NTLFM0TJ",
    "outputId": "eb9ff0ca-24c8-4b47-d27f-bb3493e6a1ac"
   },
   "outputs": [],
   "source": [
    "def reshape_for_lstm(X, time_steps):\n",
    "    num_samples = X.shape[0] // time_steps  # Ensure we can divide into sequences\n",
    "    X = X[:num_samples * time_steps]  # Trim extra rows that don't fit\n",
    "    X_reshaped = X.reshape(num_samples, time_steps, X.shape[1])  # Reshape to 3D\n",
    "    return X_reshaped\n",
    "\n",
    "# Reshape your data\n",
    "X_train_reshaped = reshape_for_lstm(X_train, time_steps)\n",
    "X_val_reshaped = reshape_for_lstm(X_val, time_steps)\n",
    "X_test_reshaped = reshape_for_lstm(X_test, time_steps)\n",
    "\n",
    "# Print new shapes\n",
    "print(\"X_train_reshaped shape:\", X_train_reshaped.shape)\n",
    "print(\"X_val_reshaped shape:\", X_val_reshaped.shape)\n",
    "print(\"X_test_reshaped shape:\", X_test_reshaped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTndTVXXStvb",
    "outputId": "1f28be4c-54dc-4fde-f58c-0805c1c0442f"
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNod5JipSwYV",
    "outputId": "1c4be92e-0c62-498e-88a9-8e11099fb5de"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LasKQPYVFo5j",
    "outputId": "1705dfce-f328-4ea9-e905-50fa202d044b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(75, 1)),  # Adjust input shape to match reshaped data\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(6)  # Output for 1h, 3h, 6h, 24h, 48h, 72h\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model using TensorFlow datasets\n",
    "history = model.fit(\n",
    "    train_dataset,  # Training data\n",
    "    validation_data=val_dataset,  # Validation data\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predict for the test dataset\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# If you need predictions for x_test (numpy array or tensor)\n",
    "x_test_reshaped = tf.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  # Reshape to 3D for LSTM\n",
    "predictions_array = model.predict(x_test_reshaped)\n",
    "\n",
    "print(\"Predictions shape:\", predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itgDUQkgG1I5",
    "outputId": "86bb046e-8a9d-42e1-b607-62c6610b405b"
   },
   "outputs": [],
   "source": [
    "# model.save(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/LSTM_regression.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j__EA6jFjcRR",
    "outputId": "645d5f79-bac1-483e-cc68-1802146eaf97"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics\n",
    "\n",
    "# Load the saved model with explicit loss and metrics\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/LSTM_regression.h5\",\n",
    "    custom_objects={\"mse\": losses.MeanSquaredError(), \"mae\": metrics.MeanAbsoluteError()}\n",
    ")\n",
    "\n",
    "# Evaluate the loaded model (optional)\n",
    "loss, mae = loaded_model.evaluate(test_dataset)\n",
    "print(f\"Loaded Model Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predict using the loaded model\n",
    "x_test_reshaped = tf.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  # Reshape to 3D for LSTM\n",
    "predictions_array = loaded_model.predict(x_test_reshaped)\n",
    "print(predictions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWtv_bPSwpRi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2sxJO695QnK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5F7nDNU85Qjh",
    "outputId": "88a4066a-f4ef-4c84-c02b-e89f5b170616"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "# Define the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(75,1)),\n",
    "    LayerNormalization(),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    LayerNormalization(),\n",
    "    tf.keras.layers.Dense(6)\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model using TensorFlow datasets\n",
    "history = model.fit(\n",
    "    train_dataset,  # Training data\n",
    "    validation_data=val_dataset,  # Validation data\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predict for the test dataset\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# If you need predictions for x_test (numpy array or tensor)\n",
    "x_test_reshaped = tf.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))  # Reshape to 3D for LSTM\n",
    "predictions_array = model.predict(x_test_reshaped)\n",
    "\n",
    "print(\"Predictions shape:\", predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb2rgG5Y5jFQ"
   },
   "outputs": [],
   "source": [
    "# model.save(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/second_LSTM_regression.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxIu5IjGS0LV",
    "outputId": "6ab55094-9e35-4d45-e202-0d6a3fbf1967"
   },
   "outputs": [],
   "source": [
    "# !pip install keras-tuner --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHSv725L_-Rk",
    "outputId": "4602caf7-7e85-4f7c-8d8f-aab7b7cafd69"
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(\n",
    "            units=hp.Int(\"units_lstm1\", min_value=32, max_value=128, step=32),  # Tune LSTM units\n",
    "            return_sequences=True,\n",
    "            input_shape=(75, 1)\n",
    "        ),\n",
    "        LayerNormalization(),\n",
    "        tf.keras.layers.LSTM(\n",
    "            units=hp.Int(\"units_lstm2\", min_value=16, max_value=64, step=16),  # Tune LSTM units\n",
    "        ),\n",
    "        LayerNormalization(),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=6\n",
    "        )\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])  # Tune learning rate\n",
    "        ),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_mae\",  # Optimize for validation MAE\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory=\"my_tuning_dir\",\n",
    "    project_name=\"lstm_tuning\"\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[stop_early]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgRFwpbdSuMX",
    "outputId": "6ca5ac86-ebaf-41d7-e4fa-fd18a84d34a0"
   },
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"LSTM1 Units: {best_hps.get('units_lstm1')}\")\n",
    "print(f\"LSTM2 Units: {best_hps.get('units_lstm2')}\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "# Build the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/best_lstm_model.keras\")\n",
    "print(\"Best model saved as 'best_lstm_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1HBloP9ew5K"
   },
   "source": [
    "### TSMixer Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929
    },
    "id": "dHv2BK5CjSfm",
    "outputId": "a341b2ce-543c-4bf0-d6ec-ebedde5d39c1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_tsmixer_model(input_shape, output_units):\n",
    "    \"\"\"\n",
    "    Builds a TSMixer model for regression.\n",
    "\n",
    "    Parameters:\n",
    "    - input_shape: Tuple representing the input shape (sequence_length, num_features).\n",
    "    - output_units: Number of regression targets (output units).\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Linear embedding of input features\n",
    "    x = layers.Dense(128, activation='relu')(inputs)  # Feature embedding\n",
    "\n",
    "    # First Mixer Layer: Time Mixing\n",
    "    x = layers.Permute((2, 1))(x)  # Permute to mix across the time dimension\n",
    "    x = layers.Dense(128, activation='relu')(x)  # Time mixing\n",
    "    x = layers.Permute((2, 1))(x)  # Restore original dimensions\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Second Mixer Layer: Feature Mixing\n",
    "    x = layers.Dense(128, activation='relu')(x)  # Feature mixing\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Global Average Pooling across the time dimension\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Output layer for regression\n",
    "    outputs = layers.Dense(output_units, activation='linear')(x)  # Linear activation for regression\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Adjust learning rate if needed\n",
    "        loss='mse',  # Mean Squared Error for regression\n",
    "        metrics=['mae']  # Mean Absolute Error as a metric\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define model parameters\n",
    "sequence_length = 75  # Length of each sequence\n",
    "num_features = 1      # Number of input features per timestep\n",
    "output_units = 6      # Number of regression targets\n",
    "\n",
    "# Build the TSMixer model\n",
    "model = build_tsmixer_model((sequence_length, num_features), output_units)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,  # Training dataset (e.g., tf.data.Dataset)\n",
    "    validation_data=val_dataset,  # Validation dataset\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=64,  # Batch size\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(x_test)\n",
    "print(\"Predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5MMQ3wde3pr"
   },
   "source": [
    "### TXMixer Hyper-prameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioObWLTSkdc2",
    "outputId": "bb5bf79d-7ec6-4a6c-dbb7-7392f8a87f9b"
   },
   "outputs": [],
   "source": [
    "# Hyperprameter tunning of TSMixer\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a function to build the TSMixer model\n",
    "def build_tsmixer_model(hp):\n",
    "    \"\"\"\n",
    "    Build a TSMixer model with tunable hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - hp: Hyperparameter tuner object.\n",
    "\n",
    "    Returns:\n",
    "    - Compiled TSMixer model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(75, 1))  # Input shape (sequence_length=75, num_features=1)\n",
    "\n",
    "    # Linear embedding of input features\n",
    "    x = layers.Dense(hp.Int(\"embedding_units\", min_value=64, max_value=256, step=64), activation='relu')(inputs)\n",
    "\n",
    "    # Add variable number of TSMixer layers\n",
    "    for i in range(hp.Int(\"num_mixer_layers\", min_value=1, max_value=4, step=1)):  # Vary the number of mixer layers\n",
    "        # Time mixing\n",
    "        x = layers.Permute((2, 1))(x)\n",
    "        x = layers.Dense(hp.Int(f\"units_time_mixer_{i+1}\", min_value=64, max_value=256, step=64), activation='relu')(x)\n",
    "        x = layers.Permute((2, 1))(x)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "        # Feature mixing\n",
    "        x = layers.Dense(hp.Int(f\"units_feature_mixer_{i+1}\", min_value=64, max_value=256, step=64), activation='relu')(x)\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Output layer for regression\n",
    "    outputs = layers.Dense(6, activation='linear')(x)  # Regression output (6 targets)\n",
    "\n",
    "    # Build the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])  # Tune learning rate\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_tsmixer_model,\n",
    "    objective=\"val_mae\",  # Minimize validation MAE\n",
    "    max_epochs=50,\n",
    "    factor=3,  # Reduction factor for the number of configurations\n",
    "    directory=\"tsmixer_tuning\",\n",
    "    project_name=\"regression_tsmixer\"\n",
    ")\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Start the hyperparameter search\n",
    "tuner.search(\n",
    "    train_dataset,  # Training dataset (tf.data.Dataset or numpy arrays)\n",
    "    validation_data=val_dataset,  # Validation dataset\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"\"\"\n",
    "The optimal number of embedding units is {best_hps.get('embedding_units')}.\n",
    "The optimal number of mixer layers is {best_hps.get('num_mixer_layers')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model with early stopping\n",
    "history = best_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "best_model.save(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/best_tsmixer_model.keras\")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "loss, mae = best_model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predictions\n",
    "predictions = best_model.predict(x_test)\n",
    "print(\"Predictions shape:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNngkQWYe9Vc"
   },
   "source": [
    "### Loading & Evaluating TSMixer Model:\n",
    "After Hyper-prameter Tunning of TSMixer Model now it is time to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "7lbVlXZAlrjO",
    "outputId": "7d78016f-60dc-4f4f-9ede-a022884fb285"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Load the saved TSMixer model\n",
    "model_path = \"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/best_tsmixer_model.keras\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRFDlzgwfjhS",
    "outputId": "cf03b3f8-b3d6-456f-fd78-e26613885f5e"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Test Loss (MSE): {loss}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPNHfyYo4n_h"
   },
   "source": [
    "### Evaluating the Model on Test data.\n",
    "The model is evaluating on Mean Squared Error and R square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVfPlOvMfxhS",
    "outputId": "990dde2e-6a63-4874-ce37-c963bfe4c1c7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "y_pred = y_pred.numpy() if isinstance(y_pred, tf.Tensor) else y_pred\n",
    "y_true = y_true.numpy() if isinstance(y_true, tf.Tensor) else y_true\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# Compute R² Score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQA1SSmXgFQY",
    "outputId": "c60e8a1c-7a2c-4f58-80b4-79ef4c706f31"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "# Convert train_dataset to numpy arrays\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for x_batch, y_batch in train_dataset:\n",
    "    x_data.append(x_batch.numpy())\n",
    "    y_data.append(y_batch.numpy())\n",
    "\n",
    "# Combine into full arrays\n",
    "x_data = np.concatenate(x_data, axis=0)\n",
    "y_data = np.concatenate(y_data, axis=0)\n",
    "\n",
    "# K-fold cross validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Store evaluation results for each fold\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(x_data):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    x_train_fold, x_val_fold = x_data[train_index], x_data[val_index]\n",
    "    y_train_fold, y_val_fold = y_data[train_index], y_data[val_index]\n",
    "\n",
    "    # Reload the model for each fold\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Train the model on the current fold\n",
    "    history = model.fit(\n",
    "        x_train_fold, y_train_fold,\n",
    "        validation_data=(x_val_fold, y_val_fold),\n",
    "        epochs=15,\n",
    "        batch_size=64,\n",
    "        verbose=1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "    # Predict on validation fold\n",
    "    y_pred_fold = model.predict(x_val_fold)\n",
    "\n",
    "    # Calculate metrics for the current fold\n",
    "    mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_val_fold, y_pred_fold)\n",
    "\n",
    "    print(f\"Fold {fold} - MSE: {mse}, MAE: {mae}, RMSE: {rmse}, R²: {r2}\")\n",
    "\n",
    "    # Append metrics to the results\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Print overall metrics across all folds\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Average MSE: {np.mean(mse_scores)}\")\n",
    "print(f\"Average MAE: {np.mean(mae_scores)}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")\n",
    "print(f\"Average R²: {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "id": "WURvWYtF5quO",
    "outputId": "bcc53ec0-ed6d-46a6-db45-b48a08d58105"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: y_true and y_pred after cross-validation\n",
    "y_true = y_val_fold.flatten()\n",
    "y_pred = model.predict(x_val_fold).flatten()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5, label=\"Predicted vs Actual\")\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', label=\"Ideal Fit\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.legend()\n",
    "plt.title(\"True vs Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "2JHDeBTVWDv9",
    "outputId": "440c297f-ab3d-430f-84f2-b904d6199274"
   },
   "outputs": [],
   "source": [
    "residuals = y_true - y_pred\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W01xqbbKWjjX"
   },
   "source": [
    "### Explainable AI:\n",
    "Using SHAP for checking the output result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrBIsWTHWbUs"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Extract data from train_dataset\n",
    "x_data = []\n",
    "for x_batch, _ in train_dataset:\n",
    "    x_data.append(x_batch.numpy())  # Convert each batch to NumPy\n",
    "\n",
    "# Concatenate all batches into a single NumPy array\n",
    "x_data = np.concatenate(x_data, axis=0)  # Combine batches along the sample axis\n",
    "x_data = x_data.reshape(-1, 75, 1)  # Reshape to match model input shape\n",
    "\n",
    "# Define the model prediction wrapper\n",
    "def model_predict(inputs):\n",
    "    # Ensure inputs are converted to the correct shape for the model\n",
    "    inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)  # Convert to Tensor\n",
    "    return model(inputs).numpy()  # Run model and convert predictions to NumPy\n",
    "\n",
    "explainer = shap.KernelExplainer(model_predict, x_data)  # Use a subset as baseline\n",
    "shap_values = explainer.shap_values(x_data)\n",
    "shap.summary_plot(shap_values, x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2dcc4354c7494254887e4fabc7696f14",
      "2bf4810073e1405cb03bf9454b495ab9",
      "b8b77f549e334f41a55d524d7ecd6b57",
      "1f97c733d4aa4fe4b75adf25e3a8696c",
      "e683e676535e4fdaba7e4ef1f34d720f",
      "6c7482b2eab6473b880af0eeed13909e",
      "daa66cf065ca42d9a7958bf4ecb2b11d",
      "c3ed5c9c9a484b199591d3e6b96dc063",
      "01cd18550c364646af7edca3c5cfd352",
      "816f00c58a4745c5967e2160306db383",
      "4e559359142943aaab43764ca14c208e"
     ]
    },
    "id": "FwOk0Lh9avuC",
    "outputId": "3028f97f-4a43-4cc1-83eb-6ba1934c00eb"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Enable NumPy-like behavior in TensorFlow\n",
    "tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
    "\n",
    "# Load your trained TensorFlow model\n",
    "model = tf.keras.models.load_model(\"/content/drive/MyDrive/AFG-FTL-Capstone-Project/Models/best_tsmixer_model.keras\")  # Replace with actual path\n",
    "\n",
    "# Prepare a subset of the test data for SHAP\n",
    "x_test_sample = x_test[:100]  # Use a smaller subset for SHAP (e.g., 100 samples)\n",
    "\n",
    "# Convert the test data to NumPy arrays\n",
    "x_test_sample_np = x_test_sample.numpy()  # Convert to NumPy\n",
    "y_test_sample_np = y_test[:100].numpy()   # Optional: Convert y_test if needed\n",
    "\n",
    "# Define a function for predictions\n",
    "# Define a prediction function\n",
    "def predict(input_data):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "# Flatten the input data\n",
    "x_test_sample_flat = x_test_sample_np.reshape(x_test_sample_np.shape[0], -1)  # Flatten timesteps and features\n",
    "\n",
    "# Initialize SHAP KernelExplainer with flattened data\n",
    "explainer = shap.KernelExplainer(predict, x_test_sample_flat)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(x_test_sample_flat)\n",
    "\n",
    "# Visualize SHAP values\n",
    "shap.summary_plot(shap_values, x_test_sample_flat)\n",
    "\n",
    "# Visualize single-instance explanations\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], x_test_sample_flat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XYuVaDXgMYP"
   },
   "source": [
    "### It is time to Train our Reinforcment Learning Model:\n",
    "In Reinforcment learning we are going to use PPO model, because it is a policy optimization and we just need that based on our policy make a decision.\n",
    "<br>\n",
    "**Steps we follow:**<br>\n",
    "* Creating Envirnoment\n",
    "* Train PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwRhBqMThxrd"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Define the custom environment\n",
    "class AirQualityEnv(gym.Env):\n",
    "    def __init__(self, regression_predictions, actual_values, external_features=None):\n",
    "        \"\"\"\n",
    "        Custom environment for PPO with air quality predictions.\n",
    "\n",
    "        :param regression_predictions: np.array, predictions from the regression model\n",
    "        :param actual_values: np.array, ground truth values for reward calculation\n",
    "        :param external_features: np.array, external features (optional, can be None)\n",
    "        \"\"\"\n",
    "        super(AirQualityEnv, self).__init__()\n",
    "\n",
    "        # Initialize predictions, actual values, and external features\n",
    "        self.regression_predictions = regression_predictions\n",
    "        self.actual_values = actual_values\n",
    "        self.external_features = external_features if external_features is not None else np.zeros((5,))\n",
    "\n",
    "        # Define the action space (adjustments to predictions)\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.regression_predictions),), dtype=np.float32)\n",
    "\n",
    "        # Define the observation space (predictions + external features)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(self.regression_predictions) + len(self.external_features),),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Combine predictions and external features into the state\n",
    "        self.state = np.concatenate([self.regression_predictions, self.external_features])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment.\n",
    "\n",
    "        :param action: np.array, adjustments to predictions\n",
    "        :return: tuple (state, reward, done, info)\n",
    "        \"\"\"\n",
    "        # Apply action (adjust predictions)\n",
    "        adjusted_predictions = self.regression_predictions + action\n",
    "\n",
    "        # Calculate reward (negative mean absolute error as an example)\n",
    "        reward = -np.mean(np.abs(adjusted_predictions - self.actual_values))\n",
    "\n",
    "        # Update state\n",
    "        self.state = np.concatenate([adjusted_predictions, self.external_features])\n",
    "\n",
    "        # End episode after one step (for simplicity)\n",
    "        done = True\n",
    "\n",
    "        # Additional info (empty for now)\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment state.\n",
    "        \"\"\"\n",
    "        self.state = np.concatenate([self.regression_predictions, self.external_features])\n",
    "        return self.state\n",
    "\n",
    "# Example data\n",
    "regression_predictions = np.array([1, 2, 3, 4, 5])  # Replace with actual predictions\n",
    "actual_values = np.array([2, 3, 4, 5, 6])           # Replace with ground truth\n",
    "external_features = np.array([0.1, 0.2, 0.3, 0.4, 0.5])  # Replace with external features if available\n",
    "\n",
    "# Instantiate the environment\n",
    "env = AirQualityEnv(regression_predictions, actual_values, external_features)\n",
    "\n",
    "# Vectorize the environment (for stability and parallel training)\n",
    "vec_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "# Create the PPO model\n",
    "ppo_model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "\n",
    "# Train the PPO model\n",
    "ppo_model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the model\n",
    "ppo_model.save(\"ppo_air_quality_model.zip\")\n",
    "\n",
    "# Load the model (if needed later)\n",
    "ppo_model = PPO.load(\"ppo_air_quality_model.zip\")\n",
    "\n",
    "# Test the model\n",
    "obs = env.reset()\n",
    "action, _ = ppo_model.predict(obs)\n",
    "adjusted_predictions = regression_predictions + action\n",
    "\n",
    "print(\"Adjusted Predictions:\", adjusted_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001061702153468f9fd7770255f864ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01cd18550c364646af7edca3c5cfd352": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "02abdd5c0ce0425c81f3163d3331efd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f855a39c8fc445ecb9a67e3aa4982ae3",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ac585964aa34317bb33c4b91deb2dce",
      "value": 5
     }
    },
    "0723a319701a4379a86a5ee174cc125d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13070a758e0e4e2bb3c849b579eb433a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14418179454043d8a01569bd17efb2a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1468bdad0bb94b2f9326f9fd106ec371": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19ba0f039a654cd9b15eeeeab582956d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1eff73fbf07b4b1394801938f9997c1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3361aaeb7be74ba3823590282ee8ecf3",
      "placeholder": "​",
      "style": "IPY_MODEL_f50909b3526240e5a34b582fa9542c60",
      "value": "Summarize dataset: 100%"
     }
    },
    "1f97c733d4aa4fe4b75adf25e3a8696c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_816f00c58a4745c5967e2160306db383",
      "placeholder": "​",
      "style": "IPY_MODEL_4e559359142943aaab43764ca14c208e",
      "value": " 62/100 [4:55:56&lt;3:20:37, 316.77s/it]"
     }
    },
    "1fc0598fc4fc4f2fb1bcb4f1bac6427b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1eff73fbf07b4b1394801938f9997c1a",
       "IPY_MODEL_adcbdc5efbe644e38024c8de4e51d7d1",
       "IPY_MODEL_985989c32a534ec1826f9f21751f8522"
      ],
      "layout": "IPY_MODEL_417912ed8b4e4dc493269c5cf09f53d9"
     }
    },
    "226b7f00a1a34f56a7964043cf0b6630": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "235120c6c15a4b98962b25b758a4435c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26451240ab1a4c15945ac45357189297": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26c02ba114b048448eddcef249a2f57c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29281ed12ab6439baf461333bd1d9ad2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bf4810073e1405cb03bf9454b495ab9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c7482b2eab6473b880af0eeed13909e",
      "placeholder": "​",
      "style": "IPY_MODEL_daa66cf065ca42d9a7958bf4ecb2b11d",
      "value": " 62%"
     }
    },
    "2c5489b54f5f40b38d579de2d3144d4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2dcc4354c7494254887e4fabc7696f14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2bf4810073e1405cb03bf9454b495ab9",
       "IPY_MODEL_b8b77f549e334f41a55d524d7ecd6b57",
       "IPY_MODEL_1f97c733d4aa4fe4b75adf25e3a8696c"
      ],
      "layout": "IPY_MODEL_e683e676535e4fdaba7e4ef1f34d720f"
     }
    },
    "300ac91f3d144d9da521261290e3580a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32ab583530d74b338acf8e713edcc4c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d2e9a68fd6604a5d927c2e1b95cf698f",
       "IPY_MODEL_f41693ca8ad949cf9760f3c13811295d",
       "IPY_MODEL_88b65cf13ee04cc293309823b8588c66"
      ],
      "layout": "IPY_MODEL_d21ad422038b48ce95d8f19f68434b6c"
     }
    },
    "33126f008ac848488c2b73d6ee785266": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3361aaeb7be74ba3823590282ee8ecf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37105ee2c88b4c5a825cc3022171755c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47dad0c29bf0462096bd9e78de498e7c",
      "placeholder": "​",
      "style": "IPY_MODEL_adf49bc535494cb98a1e55789be96321",
      "value": " 166/166 [00:43&lt;00:00,  1.85it/s, Completed]"
     }
    },
    "376856a786934340bc73306a4be2dd0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb2fde6856747a6872e3d9e1fd31cd8",
      "placeholder": "​",
      "style": "IPY_MODEL_8c452c291c254ce7a940e659e2d6b742",
      "value": "Render HTML: 100%"
     }
    },
    "390c2edfdd0c49f897802e31372f22fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39c827bad00e403585e7dc9970ea2338": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d8b57fdcf074327996b702f7257ab9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_300ac91f3d144d9da521261290e3580a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd4d1d1f852e40beb5f5633d6994853f",
      "value": 1
     }
    },
    "3e6af4fbe22f4dc8b8750a044a2bb58e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3eb6de40a18445c69fc8f6c3fc211295": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb64a0ada3cf44789f7cb09e8f8baea6",
      "placeholder": "​",
      "style": "IPY_MODEL_e10c935c79844ed7a394e0dfff6fd6bb",
      "value": " 1/1 [00:13&lt;00:00, 13.86s/it]"
     }
    },
    "3f2b388313b54be8ae152db0a680ab22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "417912ed8b4e4dc493269c5cf09f53d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41fc55a3f10f4df6b04a57d9bba7862b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44eb0f14d41d4607b7627e359d26e59f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b2d4feeed63f4404aaa3466d1b21cc1b",
       "IPY_MODEL_b3d37dd129aa400bb8272445e2677d3f",
       "IPY_MODEL_dc1825f4fef246fb9df99d530ba1d89f"
      ],
      "layout": "IPY_MODEL_19ba0f039a654cd9b15eeeeab582956d"
     }
    },
    "453f95a6c3284e5297b49c15137696df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47dad0c29bf0462096bd9e78de498e7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e559359142943aaab43764ca14c208e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5763d2305bce477b882cd8de01ac7234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3c8b36452214a0780867253b04fd0bb",
      "placeholder": "​",
      "style": "IPY_MODEL_41fc55a3f10f4df6b04a57d9bba7862b",
      "value": " 123/123 [00:34&lt;00:00,  2.55it/s, Completed]"
     }
    },
    "58e34bb37d7747358979a887b937930c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aa10c7032454e35b137872c12653e6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5c21c3a4e1984135ab878b6f4ec1c473",
       "IPY_MODEL_02abdd5c0ce0425c81f3163d3331efd7",
       "IPY_MODEL_37105ee2c88b4c5a825cc3022171755c"
      ],
      "layout": "IPY_MODEL_e479a688365b46efba5896c245058f75"
     }
    },
    "5c21c3a4e1984135ab878b6f4ec1c473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13070a758e0e4e2bb3c849b579eb433a",
      "placeholder": "​",
      "style": "IPY_MODEL_390c2edfdd0c49f897802e31372f22fa",
      "value": "Summarize dataset: 100%"
     }
    },
    "5c53113273dd4db482d9a660ebb7fb37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b45f2dc03095463e94300f1e0daf4224",
       "IPY_MODEL_3d8b57fdcf074327996b702f7257ab9b",
       "IPY_MODEL_3eb6de40a18445c69fc8f6c3fc211295"
      ],
      "layout": "IPY_MODEL_c1be10dcf3ab4a28b42acf991a3f1fce"
     }
    },
    "62493a82c55d48738a95356045a60705": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67a90a565f0f408eb89ad7814c2dbd2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7745bf5f386f4b13ba7642f477669054",
       "IPY_MODEL_bc4e2d1534d94c60bfe6665eac43ccbb",
       "IPY_MODEL_eddb1bb875fb4249bc3ee16551892a37"
      ],
      "layout": "IPY_MODEL_82cbbd9a83a149ae9e3edecc25826e6d"
     }
    },
    "6a3046ef6b354a60a454f66493687238": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26451240ab1a4c15945ac45357189297",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abf3e013de29493ab8ae2b223a4a407e",
      "value": 5
     }
    },
    "6c7482b2eab6473b880af0eeed13909e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7663f42f071d4ab18fbb89c4e8d9c117": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e270ca8a8f2945109e57f53bc2eb8e04",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ea860ab381f46269a53a2291b71f63c",
      "value": 1
     }
    },
    "7745bf5f386f4b13ba7642f477669054": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_001061702153468f9fd7770255f864ff",
      "placeholder": "​",
      "style": "IPY_MODEL_8a17b211bada4143b3109994509ababc",
      "value": "Generate report structure: 100%"
     }
    },
    "7ac585964aa34317bb33c4b91deb2dce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bd1dba9b16144bda916ff5d0fe14ab3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f6de81b902e4869815d2f640833dbc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8138042bed69456bb5e007b29c35c68b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0723a319701a4379a86a5ee174cc125d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99f48c9e7fdc4e48902d9fb3f037d7c2",
      "value": 1
     }
    },
    "816f00c58a4745c5967e2160306db383": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "827f412fd92c404d9882e86cc85dd337": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bd1dba9b16144bda916ff5d0fe14ab3",
      "placeholder": "​",
      "style": "IPY_MODEL_235120c6c15a4b98962b25b758a4435c",
      "value": "Generate report structure: 100%"
     }
    },
    "82cbbd9a83a149ae9e3edecc25826e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "881d809c358c4905b094c5baa7861510": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88b65cf13ee04cc293309823b8588c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_226b7f00a1a34f56a7964043cf0b6630",
      "placeholder": "​",
      "style": "IPY_MODEL_1468bdad0bb94b2f9326f9fd106ec371",
      "value": " 1/1 [00:03&lt;00:00,  3.63s/it]"
     }
    },
    "8a17b211bada4143b3109994509ababc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c452c291c254ce7a940e659e2d6b742": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ea860ab381f46269a53a2291b71f63c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "947ad21244f74946847a8f0af1a73dd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94e277e7d3a8478ebcfc02a9f6919b27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f6de81b902e4869815d2f640833dbc4",
      "placeholder": "​",
      "style": "IPY_MODEL_33126f008ac848488c2b73d6ee785266",
      "value": "Summarize dataset: 100%"
     }
    },
    "985989c32a534ec1826f9f21751f8522": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d26b25adf0644359b1ef7eec292d9390",
      "placeholder": "​",
      "style": "IPY_MODEL_bf54d793da20428aa1455e648c59d42a",
      "value": " 123/123 [00:59&lt;00:00,  1.01it/s, Completed]"
     }
    },
    "99f48c9e7fdc4e48902d9fb3f037d7c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9fc8c4c8d2294b85a3ede1c812e5252c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3c8b36452214a0780867253b04fd0bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaa28a96dd394f529f082ab31b092240": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abf3e013de29493ab8ae2b223a4a407e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "adcbdc5efbe644e38024c8de4e51d7d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6b11e3a8882432bb8d574f6436e2899",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b1369932757a4e93bbaf7cb24e4dfe63",
      "value": 5
     }
    },
    "adf49bc535494cb98a1e55789be96321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae5e2a10b7844395a0788362043faa3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_827f412fd92c404d9882e86cc85dd337",
       "IPY_MODEL_8138042bed69456bb5e007b29c35c68b",
       "IPY_MODEL_cb216a9109934d43be73e03e84667867"
      ],
      "layout": "IPY_MODEL_b18b23098fe840f5942746c8f0e259e6"
     }
    },
    "b1369932757a4e93bbaf7cb24e4dfe63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b18b23098fe840f5942746c8f0e259e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2d4feeed63f4404aaa3466d1b21cc1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14418179454043d8a01569bd17efb2a6",
      "placeholder": "​",
      "style": "IPY_MODEL_947ad21244f74946847a8f0af1a73dd7",
      "value": "Render HTML: 100%"
     }
    },
    "b3d37dd129aa400bb8272445e2677d3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58e34bb37d7747358979a887b937930c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ff2e2ee7304b49818de1d6cf4ebede7c",
      "value": 1
     }
    },
    "b3e7aacb44ce4f37b91fad0217fc7757": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b45f2dc03095463e94300f1e0daf4224": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_881d809c358c4905b094c5baa7861510",
      "placeholder": "​",
      "style": "IPY_MODEL_453f95a6c3284e5297b49c15137696df",
      "value": "Generate report structure: 100%"
     }
    },
    "b5edf73e0fd14b8e8cb34202f910143d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29281ed12ab6439baf461333bd1d9ad2",
      "placeholder": "​",
      "style": "IPY_MODEL_62493a82c55d48738a95356045a60705",
      "value": " 1/1 [00:03&lt;00:00,  3.52s/it]"
     }
    },
    "b8b77f549e334f41a55d524d7ecd6b57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3ed5c9c9a484b199591d3e6b96dc063",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01cd18550c364646af7edca3c5cfd352",
      "value": 62
     }
    },
    "bc4e2d1534d94c60bfe6665eac43ccbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf4a71b2cd134da9b472dbe715e9967f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2c5489b54f5f40b38d579de2d3144d4d",
      "value": 1
     }
    },
    "bd4d1d1f852e40beb5f5633d6994853f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf4a71b2cd134da9b472dbe715e9967f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf54d793da20428aa1455e648c59d42a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0aecd389f99479da98a4df0ed2e13f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0ca51e5a0134b499bf5fb5713d55eed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1be10dcf3ab4a28b42acf991a3f1fce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3ed5c9c9a484b199591d3e6b96dc063": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb216a9109934d43be73e03e84667867": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3e7aacb44ce4f37b91fad0217fc7757",
      "placeholder": "​",
      "style": "IPY_MODEL_39c827bad00e403585e7dc9970ea2338",
      "value": " 1/1 [00:08&lt;00:00,  8.92s/it]"
     }
    },
    "d21ad422038b48ce95d8f19f68434b6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d26b25adf0644359b1ef7eec292d9390": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2e9a68fd6604a5d927c2e1b95cf698f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0aecd389f99479da98a4df0ed2e13f3",
      "placeholder": "​",
      "style": "IPY_MODEL_aaa28a96dd394f529f082ab31b092240",
      "value": "Render HTML: 100%"
     }
    },
    "d6b11e3a8882432bb8d574f6436e2899": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daa66cf065ca42d9a7958bf4ecb2b11d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbb2fde6856747a6872e3d9e1fd31cd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc1825f4fef246fb9df99d530ba1d89f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e6af4fbe22f4dc8b8750a044a2bb58e",
      "placeholder": "​",
      "style": "IPY_MODEL_e758f538255c4e8d93e4dbdc59a49d98",
      "value": " 1/1 [00:06&lt;00:00,  6.64s/it]"
     }
    },
    "dc386402672843bb8e82ba03056c1586": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_376856a786934340bc73306a4be2dd0f",
       "IPY_MODEL_7663f42f071d4ab18fbb89c4e8d9c117",
       "IPY_MODEL_b5edf73e0fd14b8e8cb34202f910143d"
      ],
      "layout": "IPY_MODEL_ffb3d3bef5ca453eba6b5b0dab9daa2b"
     }
    },
    "e10c935c79844ed7a394e0dfff6fd6bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e270ca8a8f2945109e57f53bc2eb8e04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e479a688365b46efba5896c245058f75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e683e676535e4fdaba7e4ef1f34d720f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e758f538255c4e8d93e4dbdc59a49d98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb64a0ada3cf44789f7cb09e8f8baea6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecdc5600be8341d6beee2e83aa32bbdc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edb99dca49204cecbdb94497a8d6e61e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_94e277e7d3a8478ebcfc02a9f6919b27",
       "IPY_MODEL_6a3046ef6b354a60a454f66493687238",
       "IPY_MODEL_5763d2305bce477b882cd8de01ac7234"
      ],
      "layout": "IPY_MODEL_ecdc5600be8341d6beee2e83aa32bbdc"
     }
    },
    "eddb1bb875fb4249bc3ee16551892a37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26c02ba114b048448eddcef249a2f57c",
      "placeholder": "​",
      "style": "IPY_MODEL_9fc8c4c8d2294b85a3ede1c812e5252c",
      "value": " 1/1 [00:14&lt;00:00, 14.36s/it]"
     }
    },
    "f41693ca8ad949cf9760f3c13811295d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0ca51e5a0134b499bf5fb5713d55eed",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f2b388313b54be8ae152db0a680ab22",
      "value": 1
     }
    },
    "f50909b3526240e5a34b582fa9542c60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f855a39c8fc445ecb9a67e3aa4982ae3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff2e2ee7304b49818de1d6cf4ebede7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ffb3d3bef5ca453eba6b5b0dab9daa2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
